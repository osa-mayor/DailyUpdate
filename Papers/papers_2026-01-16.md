# 2026-01-16 Daily Papers (Top 5)

## 1. [STEP3-VL-10B Technical Report](https://huggingface.co/papers/2601.09668)
**Upvotes**: 147

![Thumbnail](images/STEP3-VL-10B_Technical_Report_img.jpg)

### 📌 요약
경량 오픈소스 모델 STEP3-VL-10B는 통일된 비전-언어 시너지 학습과 PaCoRe 추론 전략을 통해 10B 모델임에도 불구하고 대규모 모델을 능가하는 최고 수준의 멀티모달 지능을 구현했습니다.

### 📝 초록 (번역)
우리는 소형 효율성(compact efficiency)과 최첨단 멀티모달 지능 간의 균형점을 재정의하기 위해 설계된 경량 오픈소스 기반 모델인 STEP3-VL-10B를 소개합니다. STEP3-VL-10B는 두 가지 전략적 전환을 통해 구현되었습니다. 첫째, 언어에 정렬된 인식 인코더(Perception Encoder)를 Qwen3-8B 디코더와 통합하여 내재적인 비전-언어 시너지를 확립하는, 1.2조 개의 멀티모달 토큰에 대한 통일되고 완전히 고정되지 않은(unfrozen) 사전 학습 전략입니다. 둘째, 1,000회 이상의 강화 학습(RL) 반복을 특징으로 하는 확장된 사후 학습(post-training) 파이프라인입니다. 결정적으로, 우리는 테스트 시간 컴퓨팅을 확장하기 위해 병렬 조정 추론(Parallel Coordinated Reasoning, PaCoRe)을 구현하여 다양한 시각적 가설을 탐색하고 통합하는 확장 가능한 지각 추론에 자원을 할당합니다. 결과적으로, 100억 개의 경량 모델 규모에도 불구하고 STEP3-VL-10B는 10배에서 20배 더 큰 모델(예: GLM-4.6V-106B, Qwen3-VL-235B) 및 Gemini 2.5 Pro, Seed-1.5-VL과 같은 최고 수준의 독점 플래그십 모델과 경쟁하거나 능가합니다. 동급 최강의 성능을 제공하며, MMBench에서 92.2%, MMMU에서 80.11%를 기록하는 동시에 AIME2025에서 94.43%, MathVision에서 75.95%로 복잡한 추론에서 탁월한 성과를 보였습니다. 우리는 커뮤니티에 강력하고 효율적이며 재현 가능한 기준선을 제공하기 위해 전체 모델 스위트를 공개합니다.

### 🔑 핵심 포인트
- 언어 정렬 인식 인코더와 Qwen3-8B 디코더를 1.2조 멀티모달 토큰으로 완전히 통합 학습하여 내재적인 비전-언어 시너지를 구축했습니다.
- 테스트 시간 컴퓨팅 확장을 위해 PaCoRe(병렬 조정 추론)를 도입, 확장 가능한 지각 추론을 통해 다양한 시각적 가설을 탐색하고 통합하도록 설계되었습니다.
- 10B의 경량 모델 규모에도 불구하고, MMBench 92.2%와 MMMU 80.11%를 포함하여 10배~20배 더 큰 대규모 모델 및 최상위 독점 모델들을 능가하는 성능을 달성했습니다.

---

## 2. [Urban Socio-Semantic Segmentation with Vision-Language Reasoning](https://huggingface.co/papers/2601.10477)
**Upvotes**: 143

![Thumbnail](images/Urban_Socio-Semantic_Segmentation_with_Vision-Language_Reasoning_img.jpg)

### 📌 요약
본 연구는 기존 모델이 어려움을 겪는 도시의 사회적 의미론적 분할 문제를 해결하기 위해, SocioSeg 데이터셋을 구축하고 강화 학습 기반의 시각-언어 추론 프레임워크인 SocioReasoner를 제시한다.

### 📝 초록 (번역)
인간 활동의 중심지로서, 도시 표면은 풍부한 의미론적 개체들로 구성되어 있습니다. 위성 이미지에서 이러한 다양한 개체들을 분할하는 것은 광범위한 다운스트림 응용 분야에 매우 중요합니다. 현재의 고급 분할 모델들은 물리적 속성으로 정의된 개체(예: 건물, 수역)는 안정적으로 분할할 수 있지만, 사회적으로 정의된 범주(예: 학교, 공원)에 대해서는 여전히 어려움을 겪고 있습니다. 본 연구에서는 시각-언어 모델 추론을 통해 사회-의미론적 분할을 달성합니다. 이를 용이하게 하기 위해, 우리는 위성 이미지, 디지털 지도, 그리고 계층적 구조로 조직된 사회적 의미론적 개체의 픽셀 단위 레이블을 포함하는 새로운 자원인 Urban Socio-Semantic Segmentation 데이터셋(SocioSeg)을 소개합니다. 또한, 우리는 교차 모달 인식과 다단계 추론을 통해 사회적 의미론적 개체를 식별하고 주석을 다는 인간의 과정을 시뮬레이션하는 새로운 시각-언어 추론 프레임워크인 SocioReasoner를 제안합니다. 우리는 이 미분 불가능한 프로세스를 최적화하고 시각-언어 모델의 추론 능력을 이끌어내기 위해 강화 학습을 사용합니다. 실험은 우리의 접근 방식이 최신 모델 대비 성능 향상과 강력한 제로샷 일반화 능력을 보여줍니다. 우리의 데이터셋과 코드는 https://github.com/AMAP-ML/SocioReasoner에서 이용 가능합니다.

### 🔑 핵심 포인트
- 물리적 개체 대신 학교나 공원과 같은 사회적으로 정의된 의미론적 개체(socio-semantic entities)의 분할에 초점을 맞춘다.
- 위성 이미지, 디지털 지도, 계층적 레이블을 포함하는 새로운 SocioSeg 데이터셋과 시각-언어 추론 프레임워크인 SocioReasoner를 제시한다.
- 인간의 주석 과정을 모방한 교차 모달 인식 및 다단계 추론 과정을 강화 학습(Reinforcement Learning)을 통해 최적화하여 모델의 비가분적 추론 능력을 극대화한다.

---

## 3. [Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs](https://huggingface.co/papers/2601.08763)
**Upvotes**: 121

![Thumbnail](images/Rewarding_the_Rare_Uniqueness-Aware_RL_for_Creative_Problem_Solving_in_LLMs_img.jpg)

### 📌 요약
LLM의 RL 훈련 시 발생하는 탐색 붕괴 문제를 해결하기 위해, 희소하고 독창적인 고수준 해결 전략에 명시적으로 더 높은 보상을 부여하여 pass@k 성능과 해법 다양성을 극대화하는 '고유성 인지 강화 학습(Uniqueness-Aware RL)' 방법론을 제안한다.

### 📝 초록 (번역)
강화 학습(RL)은 대규모 언어 모델(LLM)의 후기 훈련(post-training)을 위한 핵심 패러다임이 되었으며, 특히 복잡한 추론 작업에서 중요한 역할을 하지만, 종종 '탐색 붕괴(exploration collapse)' 문제를 겪습니다. 이로 인해 정책은 소수의 지배적인 추론 패턴에 조기에 집중하게 되며, pass@1은 개선될지라도 롤아웃(rollout) 수준의 다양성과 pass@k의 이득이 제한됩니다. 우리는 이러한 실패가 솔루션 집합 전반의 다양성을 정규화하는 것이 아닌, 국소적인 토큰 행동을 정규화하는 데서 비롯된다고 주장합니다. 이를 해결하기 위해, 우리는 희소한 고수준 전략을 보이는 정답에 명시적으로 보상하는 롤아웃 수준의 목적 함수인 '고유성 인지 강화 학습(Uniqueness-Aware Reinforcement Learning)'을 제안합니다. 우리의 방법은 LLM 기반 평가자(judge)를 사용하여 표면적인 변화를 무시하고 동일한 문제에 대한 롤아웃을 고수준 해결 전략에 따라 클러스터링하며, 정책 이점(policy advantages)에 클러스터 크기의 역수를 가중치로 재조정합니다. 결과적으로, 중복되는 전략보다 정확하지만 참신한 전략이 더 높은 보상을 받게 됩니다. 수학, 물리학, 의학 추론 벤치마크 전반에 걸쳐, 우리의 접근 방식은 pass@1을 희생하지 않으면서도 대규모 샘플링 예산에서 pass@k를 일관되게 개선하고 pass@k 곡선 아래 면적(AUC@K)을 증가시키며, 탐색을 지속하고 더 다양한 해결 전략을 대규모로 발견합니다.

### 🔑 핵심 포인트
- 기존 RL이 LLM 훈련 시 국소적 토큰 행동에 집중하여 소수의 지배적인 패턴으로 탐색이 붕괴되고, 결과적으로 해법 다양성 및 pass@k 성능 향상이 제한되는 문제를 제기한다.
- 제안된 '고유성 인지 강화 학습(Uniqueness-Aware RL)'은 롤아웃 수준의 목적 함수를 사용하며, LLM 기반 평가자를 통해 유사한 해결 전략을 클러스터링한 후, 희소한(rare) 정답 전략에 더 높은 보상을 부여하여 다양성을 촉진한다.
- 본 방법론은 pass@1의 손실 없이 수학, 물리학, 의학 추론 벤치마크 전반에서 pass@k 성능을 일관되게 개선하고 AUC@K(pass@k 곡선 아래 면적)를 증가시키며, 탐색을 지속적으로 유지하고 대규모로 더 다양한 솔루션 전략을 발견한다.

---

## 4. [Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning](https://huggingface.co/papers/2601.09667)
**Upvotes**: 69

![Thumbnail](images/Collaborative_Multi-Agent_Test-Time_Reinforcement_Learning_for_Reasoning_img.jpg)

### 📌 요약
본 논문은 기존 MARL의 불안정성을 해결하기 위해, 훈련 없이 추론 시점에 구조화된 텍스트 경험을 주입하여 다중 에이전트 협력 추론의 성능과 안정성을 향상시키는 MATTRL(Multi-Agent Test-Time Reinforcement Learning) 프레임워크를 제안한다.

### 📝 초록 (번역)
다중 에이전트 시스템은 많은 애플리케이션에서 실용적인 LLM 기반 협력자로 발전했으며, 다양성과 상호 점검을 통해 견고성을 확보하고 있습니다. 그러나 다중 에이전트 강화 학습(MARL) 훈련은 리소스 집약적이며 불안정합니다. 팀원들의 상호 적응은 비정상성(non-stationarity)을 유발하고, 보상은 종종 희소하고 분산이 높기 때문입니다. 따라서 우리는 추론 시점에 구조화된 텍스트 경험을 다중 에이전트 숙고 과정에 주입하는 프레임워크인 다중 에이전트 테스트 시점 강화 학습(MATTRL)을 소개합니다. MATTRL은 다중 턴 논의를 위해 전문가들로 구성된 다중 전문가 팀을 구성하고, 테스트 시점 경험을 검색 및 통합하며, 최종 결정을 위해 합의에 도달합니다. 우리는 또한 턴 수준의 경험 풀을 구축하고 이를 대화에 다시 주입하기 위한 기여도 할당(credit assignment) 방식을 연구합니다. 의학, 수학, 교육 분야의 까다로운 벤치마크 전반에서 MATTRL은 다중 에이전트 기준선 대비 평균 3.67%, 비교 가능한 단일 에이전트 기준선 대비 8.67%의 정확도 향상을 보였습니다. 제거 연구(Ablation studies)는 다양한 기여도 할당 방식을 검토하고, 이들이 훈련 결과에 미치는 영향을 자세히 비교합니다. MATTRL은 튜닝 없이도 분포 변화에 강건한(distribution-shift-robust) 다중 에이전트 추론을 위한 안정적이고 효과적이며 효율적인 경로를 제공합니다.

### 🔑 핵심 포인트
- MATTRL은 기존 MARL 훈련의 리소스 집약성과 불안정성(비정상성 및 희소 보상 문제)을 회피하기 위해 추론 시점(Test-Time)에 구조화된 텍스트 경험을 주입하는 새로운 다중 에이전트 강화 학습 프레임워크이다.
- 프레임워크는 전문가들로 구성된 다중 에이전트 팀이 다중 턴 논의를 수행하며, 테스트 시점 경험을 검색 및 통합하여 합의에 도달하는 방식으로 작동하며, 턴 수준의 경험 풀 구축을 위한 기여도 할당 방식을 연구한다.
- MATTRL은 의학, 수학 등 주요 벤치마크에서 기존 다중 에이전트 기준선 대비 평균 3.67%의 정확도 향상을 달성하며, 튜닝 과정 없이도 분포 변화에 강건한 추론 능력을 안정적이고 효율적으로 제공한다.

---

## 5. [VIBE: Visual Instruction Based Editor](https://huggingface.co/papers/2601.02242)
**Upvotes**: 50

![Thumbnail](images/VIBE_Visual_Instruction_Based_Editor_img.jpg)

### 📌 요약
VIBE는 20억 Qwen3-VL과 16억 Sana1.5로 구성된 컴팩트한 파이프라인을 통해, 훨씬 무거운 기존 모델에 필적하거나 능가하는 품질로 높은 처리량과 강력한 원본 이미지 일관성을 갖춘 명령 기반 이미지 편집을 제공한다.

### 📝 초록 (번역)
명령 기반 이미지 편집은 생성 AI 분야에서 가장 빠르게 발전하는 영역 중 하나입니다. 지난 한 해 동안, 이 분야는 수십 개의 오픈 소스 모델과 매우 유능한 상업용 시스템이 출시되면서 새로운 수준에 도달했습니다. 하지만 현재 실세계 수준의 품질을 달성하는 오픈 소스 접근 방식은 제한적입니다. 게다가, 이러한 파이프라인에서 지배적으로 선택되는 확산 모델(diffusion backbone)은 일반적으로 60억에서 200억 개의 매개변수를 포함하여, 많은 배포 및 연구 환경에서 크고 계산 비용이 많이 듭니다. 본 논문은 편집 프로세스를 안내하는 현대적인 20억 매개변수 Qwen3-VL 모델과 이미지 생성을 위한 16억 매개변수 확산 모델 Sana1.5를 사용하는, 컴팩트하고 높은 처리량의 명령 기반 이미지 편집 파이프라인을 제시합니다. 아키텍처, 데이터 처리, 훈련 구성 및 평가에 걸친 우리의 설계 결정은 이 규모에서 가능한 주요 편집 범주 전반에 걸쳐 높은 품질을 유지하면서 저비용 추론과 엄격한 원본 일관성을 목표로 합니다. ImgEdit 및 GEdit 벤치마크에서 평가했을 때, 제안된 방법은 몇 배나 많은 매개변수와 높은 추론 비용을 가진 모델을 포함하여 상당히 무거운 기준 모델들의 성능과 일치하거나 능가하며, 속성 조정, 객체 제거, 배경 편집 및 대상 교체와 같이 입력 이미지를 보존해야 하는 편집 작업에서 특히 강력합니다. 이 모델은 24GB 이내의 GPU 메모리에 적합하며, 추가적인 추론 최적화나 증류(distillation) 없이 BF16 환경의 NVIDIA H100에서 약 4초 만에 최대 2K 해상도의 편집된 이미지를 생성합니다.

### 🔑 핵심 포인트
- 기존의 60억~200억 개 매개변수 모델보다 훨씬 작은, 20억 매개변수 Qwen3-VL과 16억 매개변수 Sana1.5를 결합하여 컴팩트하고 고효율적인 명령 기반 이미지 편집 파이프라인을 구축했다.
- ImgEdit 및 GEdit 벤치마크에서 훨씬 무거운 기준 모델들과 동등하거나 능가하는 성능을 보였으며, 특히 속성 조정, 객체 제거 등 원본 이미지 보존이 필요한 편집에서 강력한 일관성을 입증했다.
- 추가 최적화 없이도 24GB 미만의 GPU 메모리로 운영 가능하며, NVIDIA H100 환경에서 2K 해상도 이미지를 약 4초 만에 생성하는 높은 처리량과 저비용 추론을 달성했다.

---

