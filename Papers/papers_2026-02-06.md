# 2026-02-06 Daily Papers (Top 5)

## 1. [CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty](https://huggingface.co/papers/2601.22027)
**Upvotes**: 69

![Thumbnail](images/CAR-bench_Evaluating_the_Consistency_and_Limit-Awareness_of_LLM_Agents_under_Real-World_Uncertainty_img.jpg)

### 📌 요약
현실 세계의 불확실성과 도구 제한 상황을 시뮬레이션하는 새로운 벤치마크인 CAR-bench를 통해, 최신 LLM 에이전트들이 일관성과 자기 인지 능력 면에서 심각한 결함을 보이며 신뢰성 개선이 시급함을 증명했습니다.

### � 핵심 포인트
- 핵심 혁신은 현실의 불확실성(모호한 요청, 도구 제한 등)을 반영하고 58개의 복잡한 도구를 갖춘 차량 내 에이전트 평가 벤치마크인 CAR-bench를 도입한 것입니다.
- 최신 LLM 에이전트들은 모호한 요청 해결(Disambiguation) 태스크에서 일관된 성공률이 50% 미만이었으며, 능력의 한계에 직면했을 때 정보 조작(Hallucination) 및 정책 위반을 자주 저질렀습니다.
- 실시간 상호작용, 복잡한 도구 사용, 고신뢰성이 요구되는 도메인(예: 차량, 금융, 의료)에서 LLM 에이전트의 안정성과 자기 인지 능력을 연구하는 개발자 및 연구자.

### 📝 초록 (번역)
배경: 기존 LLM 에이전트 평가는 이상적인 환경에 초점을 맞추어, 실제 사용자 환경(예: 차량 내 음성 비서)에서 발생하는 불확실성이나 신뢰성 문제를 간과했습니다. 문제: 실제 환경에서는 사용자가 불완전하거나 모호한 요청을 자주 하며, 에이전트는 대화, 도구 사용, 정책 준수를 통해 이러한 내재적 불확실성을 관리해야 합니다. 해결책: 우리는 일관성(Consistency), 불확실성 처리(Uncertainty Handling), 능력 인식(Capability Awareness)을 평가하기 위한 새로운 벤치마크인 CAR-bench를 제안합니다. 이 벤치마크는 LLM 기반 사용자 시뮬레이터와 내비게이션, 차량 제어 등 58개의 복잡하게 연결된 도구를 갖춘 인카(In-Car) 환경을 구현했습니다. 결과: 초기 실험 결과, 최신 추론 LLM조차도 모호성을 해결해야 하는 Disambiguation 태스크에서 50% 미만의 일관된 성공률을 보였으며, 능력의 한계에 직면했을 때 정책을 위반하거나 정보를 조작하는(Hallucination) 현상이 빈번하게 발생하여, 실세계 에이전트의 신뢰성과 자기 인지 능력 향상이 필수적임을 시사합니다.


---

## 2. [Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening](https://huggingface.co/papers/2602.05386)
**Upvotes**: 63

![Thumbnail](images/Spider-Sense_Intrinsic_Risk_Sensing_for_Efficient_Agent_Defense_with_Hierarchical_Adaptive_Screening_img.jpg)

### 📌 요약
자율 에이전트의 보안 검사 방식을 의무적 절차에서 벗어나 내재적 위험 감지 기반의 선택적 방어 방식으로 전환함으로써, 정확도와 효율성을 획기적으로 향상시킨 방어 프레임워크를 제안했습니다.

### � 핵심 포인트
- 핵심 혁신: 의무적 검사 대신, 위험 감지 시에만 작동하는 이벤트 기반 내재적 위험 감지(IRS) 및 효율성을 극대화한 계층적 방어 구조.
- 성능 및 결과: 경쟁 모델 대비 가장 낮은 공격 성공률(ASR) 및 오탐률(FPR)을 달성했으며, 시스템 지연 시간은 8.3% 미만의 최소 오버헤드만 발생.
- 적용 대상: 강력하고 효율적인 보안이 필수적인 자율 에이전트(Autonomous Agent) 개발자 및 AI 보안 연구자.

### 📝 초록 (번역)
LLM(대규모 언어 모델) 기반의 에이전트가 실생활에 적용되면서 그 유용성이 커지고 있지만, 동시에 새로운 보안 위험에 노출되고 있습니다. 기존의 방어 메커니즘은 에이전트의 특정 단계마다 보안 검사를 강제하는 '의무적 확인' 방식을 채택하여 비효율적이라는 단점이 있었습니다. 이 문제를 해결하기 위해, 우리는 에이전트가 평소 잠재적 경계를 유지하다가 위험이 감지될 때만 방어를 작동시키는 '내재적 위험 감지(IRS)' 기반의 *Spider-Sense* 프레임워크를 제안합니다. *Spider-Sense*는 위험 감지 시 알려진 패턴은 경량화된 유사성 매칭으로 빠르게 처리하고, 모호한 사례는 심층 내부 추론으로 전환하는 계층적 방어 시스템을 활용하여 외부 모델 의존 없이 효율성과 정밀도를 동시에 확보합니다. 새로운 벤치마크(S^2Bench)를 통한 광범위한 실험 결과, *Spider-Sense*는 기존 방법론 대비 가장 낮은 공격 성공률(ASR)과 오탐률(FPR)을 달성했으며, 시스템 지연 시간은 단 8.3%만 증가하여 뛰어난 성능 대비 효율성을 입증했습니다.


---

## 3. [Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR](https://huggingface.co/papers/2602.05261)
**Upvotes**: 45

![Thumbnail](images/Length-Unbiased_Sequence_Policy_Optimization_Revealing_and_Controlling_Response_Length_Variation_in__img.jpg)

### 📌 요약
기존 RLVR 알고리즘의 응답 길이 편향 문제를 이론적으로 해결하고, 이를 바탕으로 LUSPO를 제안하여 수학 및 멀티모달 추론 벤치마크에서 SOTA 성능을 달성했습니다.

### � 핵심 포인트
- RLVR의 응답 길이 변화에 대한 이론적 분석 제공 및 길이 편향을 해결한 **LUSPO** 알고리즘 제안.
- 수학 및 멀티모달 추론 벤치마크에서 기존 GSPO/GRPO 대비 일관되게 뛰어난 성능을 달성하며 새로운 SOTA 등극.
- RLVR 기반 LLM/VLM의 추론 능력 향상 및 정책 최적화 안정성 확보에 관심 있는 AI 개발자 및 연구자.

### 📝 초록 (번역)
최근 LLM 및 VLM의 추론 능력 향상을 위해 RLVR(검증 가능한 보상을 활용한 강화 학습)이 널리 사용되고 있으며, 응답 길이가 길어지는 것이 추론 능력 향상의 핵심 요소로 간주됩니다. **(배경)** 하지만 알고리즘별로 응답 길이가 변화하는 패턴이 상이하며, 특히 기존의 주요 알고리즘(GSPO 등)에는 응답 길이에 대한 편향이 내재되어 있어 성능 저하 및 '길이 붕괴' 문제를 유발했습니다. **(문제)** 본 논문은 이러한 응답 길이 변화의 근본적인 요인에 대한 심층적인 이론 분석을 제시하고, 이를 바탕으로 **LUSPO(Length-Unbiased Sequence Policy Optimization)** 알고리즘을 제안합니다. LUSPO는 GSPO의 손실 함수에 내재된 길이 편향을 제거하여 정책을 길이와 무관하게 최적화하고 학습 안정성을 크게 높입니다. **(해결책)** 광범위한 수학 추론 및 멀티모달 추론 시나리오에서 LUSPO는 기존 GRPO, GSPO와 같은 방법론 대비 지속적으로 우수한 성능을 보여주며 새로운 최첨단(SOTA) 최적화 전략임을 입증했습니다. **(결과)**


---

## 4. [MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents](https://huggingface.co/papers/2602.02474)
**Upvotes**: 42

![Thumbnail](images/MemSkill_Learning_and_Evolving_Memory_Skills_for_Self-Evolving_Agents_img.jpg)

### 📌 요약
정적인 메모리 관리 방식을 학습 및 진화 가능한 '메모리 스킬'로 대체하는 폐쇄 루프 시스템 MemSkill을 제안하여, 복잡한 환경에서 LLM 에이전트의 메모리 적응성과 성능을 획기적으로 향상시켰습니다.

### � 핵심 포인트
- 핵심 혁신 (Core Innovation): 학습 및 진화 가능한 메모리 스킬 도입: 컨트롤러, 실행기, 디자이너로 구성된 폐쇄 루프 시스템을 통해 메모리 스킬 셋 자체를 지속적으로 개선하고 '자기 진화'하는 메모리 관리를 구현함.
- 성능 및 결과 (Performance/Results): 다양한 벤치마크(LoCoMo, LongMemEval 등 장기 메모리 및 복잡한 작업 환경)에서 기존의 강력한 기준선 대비 탁월한 작업 성능 향상을 달성하며 범용성을 입증함.
- 주요 응용 대상 (Target/Application): 장기 컨텍스트 상호작용 및 복잡한 환경에서 강력하고 적응적인 메모리 관리 시스템, 즉 자가 진화형(Self-Evolving) LLM 에이전트 구축에 관심 있는 개발자 및 연구자.

### 📝 초록 (번역)
기존 대규모 언어 모델(LLM) 에이전트의 메모리 시스템은 정보 추출 및 정리를 위해 소수의 고정된(수작업으로 설계된) 절차에 의존합니다. 이러한 정적인 방식은 인간의 초기 지식에 경직되어 있어 다양한 상호작용 패턴에서 비효율적이며 장기 기록 관리에 취약했습니다. 

이러한 문제를 해결하기 위해, 본 논문은 메모리 작업을 학습하고 진화시킬 수 있는 '메모리 스킬(MemSkill)' 개념을 도입합니다. MemSkill은 메모리 작업을 추출, 통합, 가지치기하는 구조화되고 재사용 가능한 루틴으로 재구성합니다. 시스템은 '컨트롤러'가 상황에 맞는 관련 스킬을 선택하고 '실행기'가 스킬 지침에 따라 메모리를 생성합니다. 특히 MemSkill은 선택된 스킬이 잘못되거나 불완전한 메모리를 생성하는 어려운 케이스를 주기적으로 검토하고, 스킬을 개선하거나 새로운 스킬을 제안하는 '디자이너'를 추가하여 스킬 셋 자체를 진화시키는 폐쇄 루프 구조를 구현합니다.

실험 결과, MemSkill은 LoCoMo, LongMemEval, HotpotQA, ALFWorld 등 여러 벤치마크에서 강력한 기준선 대비 뛰어난 작업 수행 능력을 보였으며, LLM 에이전트가 스스로 적응하고 진화하는 메모리 관리가 가능함을 입증했습니다. 이는 더욱 적응적인 에이전트 설계를 위한 중요한 통찰을 제공합니다.


---

## 5. [Context Forcing: Consistent Autoregressive Video Generation with Long Context](https://huggingface.co/papers/2602.06028)
**Upvotes**: 29

![Thumbnail](images/Context_Forcing_Consistent_Autoregressive_Video_Generation_with_Long_Context_img.jpg)

### 📌 요약
기존의 단기 컨텍스트 한계를 넘어, 'Context Forcing' 프레임워크를 통해 20초 이상의 극도로 긴 컨텍스트를 확보하여 비디오 생성의 장기적 일관성을 획기적으로 개선했습니다.

### � 핵심 포인트
- 핵심적인 '학생-교사 불일치' 문제를 해소하고, 교사가 전체 생성 기록을 활용하도록 만든 'Context Forcing' 프레임워크.
- 기존 대비 2~10배 긴 20초 이상의 유효 컨텍스트 길이를 확보했으며, 장기 비디오 생성의 일관성 지표에서 SOTA를 능가함.
- 장기간에 걸친 시간적 일관성(Temporal Consistency) 유지가 필수적인 AI 비디오 생성 및 스트리밍 모델 연구자 및 개발자.

### 📝 초록 (번역)
최근 실시간 장기 비디오 생성 모델들은 스트리밍 방식(streaming tuning)을 사용하는데, 이때 전체 기록을 보지 못하는 짧은 컨텍스트의 '교사'가 긴 컨텍스트의 '학생'을 지도합니다. 이 구조적 불일치(학생-교사 미스매치) 때문에 교사가 전역적인 시간 의존성을 가르칠 수 없어 학생 모델의 실질적인 컨텍스트 길이가 5초 이상으로 확장되지 못하는 문제가 발생했습니다.

이 문제를 해결하기 위해, 우리는 'Context Forcing'이라는 새로운 프레임워크를 제안합니다. 이 방식은 교사 모델 역시 전체 생성 이력을 인식하게 함으로써 근본적인 지도 불일치를 제거하고, 장기적인 일관성을 학습할 수 있도록 합니다. 2분(120초) 같은 극단적인 장기 컨텍스트를 연산적으로 효율화하기 위해, 시각적 중복성을 크게 줄이는 'Slow-Fast 메모리 아키텍처' 기반의 컨텍스트 관리 시스템을 추가로 도입했습니다.

결과적으로, Context Forcing은 기존의 LongLive나 Infinite-RoPE와 같은 최신 방법론 대비 2~10배 긴 20초 이상의 유효 컨텍스트 길이를 달성했으며, 다양한 장기 비디오 평가 지표에서 기존 SOTA를 압도하는 뛰어난 일관성을 유지함을 입증했습니다.


---

