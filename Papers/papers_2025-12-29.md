# 2025-12-29 Daily Papers (Top 5)

## 1. [InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion](https://huggingface.co/papers/2512.17504)
**Upvotes**: 72

![Thumbnail](images/InsertAnywhere_Bridging_4D_Scene_Geometry_and_Diffusion_Models_for_Realistic_Video_Object_Insertion_img.jpg)

### 📌 요약
비디오 객체 삽입(VOI)의 기하학적 일관성과 시각적 충실도를 높이기 위해 4D 장면 기하학 기반 마스크 생성과 확산 모델을 결합한 InsertAnywhere 프레임워크를 제안한다.

### 🔑 핵심 포인트
- 4D 인지 마스크 생성 모듈을 사용하여 장면 기하학을 재구성하고, 시간적 일관성과 가림(occlusion) 일관성을 유지하며 사용자 지정 객체 위치를 프레임별로 일관되게 전파한다.
- 확산 기반 비디오 생성 모델을 확장하여 삽입된 객체뿐만 아니라 조명(illumination) 및 음영(shading)과 같은 주변 환경의 국부적인 변화를 공동으로 합성하여 시각적 사실성을 극대화한다.
- 지도 학습을 위해 기존 ROSE 객체 제거 데이터셋을 조명 인지 합성 데이터셋(ROSE++)으로 변환하였으며, 객체 제거 영상, 객체 포함 영상, VLM 생성 참조 이미지의 삼중쌍(triplets) 형태로 구축하였다.

---

## 2. [Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding](https://huggingface.co/papers/2512.17220)
**Upvotes**: 69

![Thumbnail](images/Mindscape-Aware_Retrieval_Augmented_Generation_for_Improved_Long_Context_Understanding_img.jpg)

### 📌 요약
인간의 '마인드스케이프 인식 능력'에서 영감을 받아, 계층적 요약으로 구축된 전역적 의미 표현을 검색 및 생성에 활용하여 장문 이해 능력을 향상시킨 MiA-RAG 시스템을 제안한다.

### 🔑 핵심 포인트
- 기존 RAG 시스템의 장문 이해 한계를 극복하기 위해 인간이 사용하는 총체적 의미 표현 능력(Mindscape-Aware Capability)을 도입했다.
- MiA-RAG는 계층적 요약을 통해 '마인드스케이프'(전역적 문맥)를 명시적으로 구축하고, 이 정보를 사용하여 검색(질의 임베딩 강화)과 생성(일관된 추론) 모두를 조건화한다.
- 다양한 장문 및 이중 언어 벤치마크에서 기준선 성능을 일관되게 능가하며, 국소적 세부 사항을 일관된 전역적 표현과 일치시켜 인간과 유사한 장문 추론 능력을 입증했다.

---

## 3. [MAI-UI Technical Report: Real-World Centric Foundation GUI Agents](https://huggingface.co/papers/2512.22047)
**Upvotes**: 21

![Thumbnail](images/MAI-UI_Technical_Report_Real-World_Centric_Foundation_GUI_Agents_img.jpg)

### 📌 요약
MAI-UI는 고급 데이터 파이프라인, 장치-클라우드 협업 시스템 및 온라인 RL 프레임워크를 통해 현실적인 배포 문제를 해결하고, GUI 그라운딩 및 모바일 내비게이션에서 새로운 SOTA를 달성한 기반 GUI 에이전트 제품군이다.

### 🔑 핵심 포인트
- MAI-UI는 2B부터 235B-A22B까지 다양한 규모의 에이전트군을 제시하며, 사용자 상호작용 및 MCP 도구 호출을 포함하는 자체 진화형 데이터 파이프라인과 네이티브 장치-클라우드 협업 시스템을 통해 현실적인 배포 문제를 해결한다.
- GUI 그라운딩(ScreenSpot-Pro 73.5%, OSWorld-G 70.9% 등) 및 모바일 GUI 내비게이션(AndroidWorld 76.7%) 벤치마크에서 새로운 SOTA를 수립했으며, Gemini-3-Pro 및 Seed1.8과 같은 경쟁 모델들을 능가하는 성능을 보였다.
- 병렬 환경 확장(32개→512개)을 통한 온라인 RL 프레임워크의 최적화로 성능을 크게 향상시켰으며, 네이티브 장치-클라우드 협업 시스템을 통해 온디바이스 성능 33% 향상 및 클라우드 모델 호출 40% 이상 절감 효과를 달성했다.

---

## 4. [UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture](https://huggingface.co/papers/2512.21675)
**Upvotes**: 20

![Thumbnail](images/UniPercept_Towards_Unified_Perceptual-Level_Image_Understanding_across_Aesthetics,_Quality,_Structur_img.jpg)

### 📌 요약
MLLM의 지각 수준 이미지 이해 능력 향상을 위해 심미성, 품질, 구조, 질감을 통합적으로 다루는 벤치마크(UniPercept-Bench)와 태스크 정렬 RL로 학습된 강력한 베이스라인 모델(UniPercept)을 제안합니다.

### 🔑 핵심 포인트
- 심미성, 품질, 구조, 질감 전반에 걸친 지각 수준 이미지 이해를 위한 통합 프레임워크인 UniPercept-Bench를 구축하고 계층적 정의 시스템 및 대규모 데이터셋을 확립함.
- 도메인 적응형 사전 학습(DAPT)과 태스크 정렬 강화 학습(Task-Aligned RL)을 적용하여 시각적 평가(VR)와 시각적 질의응답(VQA) 모두에서 강력한 일반화 성능을 보이는 UniPercept 베이스라인 모델을 개발함.
- UniPercept은 기존 MLLM 대비 지각 수준 이해에서 우수한 성능을 보이며, 텍스트-이미지 생성 모델을 위한 플러그 앤 플레이 보상 모델로 활용될 수 있음.

---

## 5. [ProEdit: Inversion-based Editing From Prompts Done Right](https://huggingface.co/papers/2512.22118)
**Upvotes**: 12

![Thumbnail](images/ProEdit_Inversion-based_Editing_From_Prompts_Done_Right_img.jpg)

### 📌 요약
ProEdit은 역변환 기반 편집에서 원본 이미지에 대한 과도한 의존성을 해결하기 위해 어텐션 영역의 KV-mix와 잠재 공간의 Latents-Shift를 제안하여 이미지 및 비디오 편집 벤치마크에서 SOTA 성능을 달성한 방법론이다.

### 🔑 핵심 포인트
- 기존 역변환 기반 편집 방법론이 원본 정보에 과도하게 의존하여 대상 객체의 속성(자세, 수, 색상 등) 변화를 제대로 반영하지 못하는 문제를 해결한다.
- 어텐션 측면에서 KV-mix를 도입하여 편집 영역 내에서 원본 및 목표 피처의 KV를 혼합함으로써 배경 일관성을 유지하면서도 원본 이미지의 영향을 줄인다.
- 잠재 공간 측면에서 Latents-Shift를 제안하여 원본 잠재 벡터의 편집 영역을 교란시키고, 샘플링 과정에서 역변환된 잠재 벡터의 간섭 영향을 제거한다.

---

