{
  "source": "huggingface_papers_weekly_copy",
  "date": "2026-02-13",
  "generated_at": "2026-02-16T05:25:20.348677+09:00",
  "count": 5,
  "chat_message": "ğŸ”¥ HF ì˜¤ëŠ˜ì˜ ë…¼ë¬¸ (2026-02-13)\n\nğŸ“ 1. The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies\nğŸ“ ìš”ì•½: ìš”ì•½ ì •ë³´ ì—†ìŒ\nğŸ”— https://huggingface.co/papers/2602.09877\n\nğŸ“ 2. Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models\nğŸ“ ìš”ì•½: Pass rate 1ì¸ ì‰¬ìš´ í”„ë¡¬í”„íŠ¸ë¥¼ í™œìš©í•˜ì—¬ LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” Composition-RL ê¸°ë²•ì„ ì œì•ˆí•˜ë©°, íŠ¹íˆ ë„ë©”ì¸ ê°„ ì¡°í•©ì„ í†µí•´ ì„±ëŠ¥ í–¥ìƒì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŒ\nğŸ”— https://huggingface.co/papers/2602.12036\n\nğŸ“ 3. DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing\nğŸ“ ìš”ì•½: 5B íŒŒë¼ë¯¸í„°ì˜ ê²½ëŸ‰í™”ëœ í†µí•© ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ DeepGen 1.0ì„ í†µí•´ ì´ë¯¸ì§€ ìƒì„± ë° í¸ì§‘ ë¶„ì•¼ì—ì„œ ë” í° ëª¨ë¸ë“¤ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±, ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œí•˜ì—¬ ì ‘ê·¼ì„±ì„ ë†’ì„.\nğŸ”— https://huggingface.co/papers/2602.12205\n\nğŸ‘‰ ì „ì²´ ë¦¬í¬íŠ¸ëŠ” GitHubì—ì„œ!",
  "items": [
    {
      "rank": 1,
      "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies",
      "paper_id": "2602.09877",
      "url": "https://huggingface.co/papers/2602.09877",
      "upvotes": 184,
      "summary": null,
      "key_points": [],
      "tags": [
        "Agent",
        "Safety"
      ],
      "confidence": "ì¤‘",
      "adoption_complexity": "ì¤‘",
      "practical_relevance": null,
      "recommended_actions": [],
      "risks_and_limits": [],
      "uncertainty_note": "ì´ˆë¡(ìš”ì•½) ê¸°ë°˜ ì •ë¦¬ì´ë¯€ë¡œ, ì„¸ë¶€ êµ¬í˜„/ì‹¤í—˜ ì„¸íŒ…ì€ ì›ë¬¸ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.",
      "repeated_in_window": false,
      "repeated_days_ago": null
    },
    {
      "rank": 2,
      "title": "Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models",
      "paper_id": "2602.12036",
      "url": "https://huggingface.co/papers/2602.12036",
      "upvotes": 88,
      "summary": "Pass rate 1ì¸ ì‰¬ìš´ í”„ë¡¬í”„íŠ¸ë¥¼ í™œìš©í•˜ì—¬ LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” Composition-RL ê¸°ë²•ì„ ì œì•ˆí•˜ë©°, íŠ¹íˆ ë„ë©”ì¸ ê°„ ì¡°í•©ì„ í†µí•´ ì„±ëŠ¥ í–¥ìƒì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŒ",
      "key_points": [
        "Pass rate 1ì¸ í”„ë¡¬í”„íŠ¸ë¥¼ í™œìš©í•˜ëŠ” Composition-RL ê¸°ë²• ì œì•ˆ",
        "ë¬¸ì œ ì¡°í•©ì„ í†µí•´ íš¨ê³¼ì ì¸ í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ë°ì´í„° í™œìš©ë¥  í–¥ìƒ",
        "ë‹¤ì–‘í•œ ëª¨ë¸ í¬ê¸°(4B~30B)ì—ì„œ ì¶”ë¡  ëŠ¥ë ¥ ê°œì„  íš¨ê³¼ ì…ì¦"
      ],
      "tags": [
        "Reinforcement Learning",
        "LLM",
        "Prompting",
        "Reasoning",
        "Evaluation"
      ],
      "confidence": "ì¤‘",
      "adoption_complexity": "ì¤‘",
      "practical_relevance": "LLM ê¸°ë°˜ ì„œë¹„ìŠ¤ë¥¼ ê°œë°œí•  ë•Œ, ê¸°ì¡´ ë°ì´í„°ì…‹ì˜ ì‰¬ìš´ í”„ë¡¬í”„íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ ê°œì„ í•˜ê³ , ë‹¤ì–‘í•œ ë„ë©”ì¸ì˜ ë°ì´í„°ë¥¼ ìœµí•©í•˜ì—¬ ë”ìš± ê°•ë ¥í•œ ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ë° í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
      "recommended_actions": [
        "ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ ë°ì´í„°ì…‹ì—ì„œ pass rate 1ì¸ í”„ë¡¬í”„íŠ¸ì˜ ë¹„ìœ¨ ë¶„ì„",
        "Composition-RLì„ ì ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒ ì‹¤í—˜",
        "ë‹¤ì–‘í•œ ë„ë©”ì¸ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°í•©í•˜ì—¬ cross-domain RL ì„±ëŠ¥ í‰ê°€"
      ],
      "risks_and_limits": [
        "ë¬¸ì œ ì¡°í•© ë°©ì‹ì— ë”°ë¼ ì„±ëŠ¥ ë³€í™”ê°€ ìˆì„ ìˆ˜ ìˆìŒ",
        "ìƒˆë¡œìš´ ì¡°í•©ëœ ë¬¸ì œì— ëŒ€í•œ ê²€ì¦ í•„ìš”"
      ],
      "uncertainty_note": "ì´ˆë¡ë§Œìœ¼ë¡œëŠ” Composition-RLì˜ êµ¬ì²´ì ì¸ ë¬¸ì œ ì¡°í•© ë°©ì‹ê³¼ ê·¸ íš¨ê³¼ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì„ íŒŒì•…í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.",
      "repeated_in_window": false,
      "repeated_days_ago": null
    },
    {
      "rank": 3,
      "title": "DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing",
      "paper_id": "2602.12205",
      "url": "https://huggingface.co/papers/2602.12205",
      "upvotes": 73,
      "summary": "5B íŒŒë¼ë¯¸í„°ì˜ ê²½ëŸ‰í™”ëœ í†µí•© ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ DeepGen 1.0ì„ í†µí•´ ì´ë¯¸ì§€ ìƒì„± ë° í¸ì§‘ ë¶„ì•¼ì—ì„œ ë” í° ëª¨ë¸ë“¤ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±, ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œí•˜ì—¬ ì ‘ê·¼ì„±ì„ ë†’ì„.",
      "key_points": [
        "5B íŒŒë¼ë¯¸í„°ì˜ ê²½ëŸ‰í™”ëœ í†µí•© ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ DeepGen 1.0 ê°œë°œ",
        "VLM ë ˆì´ì–´ íŠ¹ì§• ì¶”ì¶œ ë° 'think token' ìœµí•©ì„ ìœ„í•œ SCB í”„ë ˆì„ì›Œí¬ ë„ì…",
        "ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ê¸°ë°˜ì˜ 3ë‹¨ê³„ í•™ìŠµ ì „ëµ(ì •ë ¬ ì‚¬ì „ í•™ìŠµ, ê³µë™ ì§€ë„ í•™ìŠµ, ê°•í™” í•™ìŠµ) ì ìš©"
      ],
      "tags": [
        "Vision",
        "Multimodal",
        "Generation",
        "Editing",
        "Lightweight",
        "RAG",
        "Reasoning",
        "Benchmark",
        "Safety"
      ],
      "confidence": "ì¤‘",
      "adoption_complexity": "ì¤‘",
      "practical_relevance": "DeepGen 1.0ì€ ì ì€ ë¦¬ì†ŒìŠ¤ë¡œë„ ê³ ì„±ëŠ¥ ì´ë¯¸ì§€ ìƒì„± ë° í¸ì§‘ ê¸°ëŠ¥ì„ êµ¬í˜„í•  ìˆ˜ ìˆê²Œ í•´ì£¼ë©°, ì˜¤í”ˆì†ŒìŠ¤ë¡œ ì œê³µë˜ì–´ ì—°êµ¬ ë° ê°œë°œì— ë¹ ë¥´ê²Œ ì ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ ì‹¤ë¬´ì  ê°€ì¹˜ê°€ ë†’ìŠµë‹ˆë‹¤.",
      "recommended_actions": [
        "DeepGen 1.0 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±/í¸ì§‘ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸",
        "ì œê³µë˜ëŠ” ë°ì´í„°ì…‹ ë° í•™ìŠµ ì½”ë“œë¥¼ í™œìš©í•˜ì—¬ íŠ¹ì • ë„ë©”ì¸ì— ë§ê²Œ Fine-tuning",
        "SCB í”„ë ˆì„ì›Œí¬ë¥¼ ë¶„ì„í•˜ì—¬ ê¸°ì¡´ ëª¨ë¸ì— ì ìš© ê°€ëŠ¥ì„± ê²€í† "
      ],
      "risks_and_limits": [
        "50M ìƒ˜í”Œë¡œ í•™ìŠµë˜ì—ˆê¸° ë•Œë¬¸ì— íŠ¹ì • ë°ì´í„°ì…‹ì— ëŒ€í•œ í¸í–¥ì´ ì¡´ì¬í•  ìˆ˜ ìˆìŒ",
        "ê²½ëŸ‰ ëª¨ë¸ì´ë¯€ë¡œ ë§¤ìš° ë³µì¡í•˜ê±°ë‚˜ íŠ¹ìˆ˜í•œ ì´ë¯¸ì§€ ìƒì„±/í¸ì§‘ ì‘ì—…ì—ëŠ” í•œê³„ê°€ ìˆì„ ìˆ˜ ìˆìŒ"
      ],
      "uncertainty_note": "ì´ˆë¡ë§Œìœ¼ë¡œëŠ” ëª¨ë¸ì˜ ì‹¤ì œ ì„±ëŠ¥ ë° ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œì˜ ì•ˆì •ì„±ì„ ì™„ë²½í•˜ê²Œ íŒŒì•…í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.",
      "repeated_in_window": false,
      "repeated_days_ago": null
    },
    {
      "rank": 4,
      "title": "Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation",
      "paper_id": "2602.12125",
      "url": "https://huggingface.co/papers/2602.12125",
      "upvotes": 56,
      "summary": "Reward Extrapolationì„ í†µí•´ On-Policy Distillation ì„±ëŠ¥ì„ ê°œì„ í•˜ê³ , Teacher ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë›°ì–´ë„˜ëŠ” ë°©ë²•ë¡  ì œì‹œ",
      "key_points": [
        "OPDë¥¼ dense KL-constrained RL ê´€ì ì—ì„œ ì¬í•´ì„ ë° ì¼ë°˜í™”",
        "Reward Extrapolation (ExOPD)ì„ í†µí•´ distillation ì„±ëŠ¥ í–¥ìƒ ë° teacher ì„±ëŠ¥ ì´ˆì›”",
        "Strong-to-weak distillationì—ì„œ reward correctionì„ í†µí•œ ì„±ëŠ¥ í–¥ìƒ"
      ],
      "tags": [
        "RL",
        "Distillation",
        "Knowledge Distillation",
        "Reward Shaping",
        "Reasoning"
      ],
      "confidence": "ì¤‘",
      "adoption_complexity": "ì¤‘",
      "practical_relevance": "Distillation ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼œ ëª¨ë¸ ì••ì¶• ë° ê²½ëŸ‰í™”ë¥¼ í†µí•´ ë°°í¬ ë¹„ìš©ì„ ì¤„ì´ê³  ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ì—¬ëŸ¬ domain expertì˜ ì§€ì‹ì„ íš¨ê³¼ì ìœ¼ë¡œ í†µí•©í•˜ì—¬ ë‹¨ì¼ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ëŠ” ë° í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.",
      "recommended_actions": [
        "ê¸°ì¡´ OPD íŒŒì´í”„ë¼ì¸ì— G-OPD í”„ë ˆì„ì›Œí¬ ì ìš© ë° reward scaling factor íŠœë‹",
        "Strong-to-weak distillation í™˜ê²½ì—ì„œ teacherì˜ pre-RL ëª¨ë¸ì„ í™œìš©í•œ reward correction ì‹¤í—˜",
        "ë‹¤ì–‘í•œ teacher-student ì‚¬ì´ì¦ˆ ì¡°í•©ì—ì„œ ExOPD íš¨ê³¼ ê²€ì¦"
      ],
      "risks_and_limits": [
        "Reward scaling factor íŠœë‹ì— ë”°ë¥¸ ì¶”ê°€ì ì¸ ì‹¤í—˜ ë¹„ìš© ë°œìƒ ê°€ëŠ¥ì„±",
        "Teacherì˜ pre-RL ëª¨ë¸ì— ì ‘ê·¼ì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš° reward correction ì ìš© ë¶ˆê°€"
      ],
      "uncertainty_note": "ì´ˆë¡ë§Œìœ¼ë¡œëŠ” G-OPDì˜ ì‹¤ì œ êµ¬í˜„ ë³µì¡ë„ ë° computational overheadë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.",
      "repeated_in_window": false,
      "repeated_days_ago": null
    },
    {
      "rank": 5,
      "title": "GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning",
      "paper_id": "2602.12099",
      "url": "https://huggingface.co/papers/2602.12099",
      "upvotes": 47,
      "summary": "ì›¹ ìŠ¤ì¼€ì¼ ë¹„ë””ì˜¤ë¡œ ì‚¬ì „ í•™ìŠµëœ ì›”ë“œ ëª¨ë¸ ê¸°ë°˜ ê°•í™” í•™ìŠµì„ í†µí•´ ë¡œë´‡ íŒ”ì˜ ì¥ê¸° ì‹¤í–‰ ëŠ¥ë ¥ì„ íšê¸°ì ìœ¼ë¡œ ê°œì„ í•œ GigaBrain-0.5M* ëª¨ë¸ì„ ì œì‹œí•©ë‹ˆë‹¤.",
      "key_points": [
        "ì›¹ ìŠ¤ì¼€ì¼ ë¹„ë””ì˜¤ë¡œ ì‚¬ì „ í•™ìŠµëœ ì›”ë“œ ëª¨ë¸ì„ VLAì— í†µí•©",
        "RAMP (Reinforcement leArning via world Model-conditioned Policy)ë¥¼ í†µí•œ cross-task ì ì‘ë ¥ í–¥ìƒ",
        "Laundry Folding, Box Packing, Espresso Preparation ë“± ë³µì¡í•œ ì¡°ì‘ ì‘ì—…ì—ì„œ 30% ì„±ëŠ¥ í–¥ìƒ"
      ],
      "tags": [
        "Robotics",
        "Reinforcement Learning",
        "World Model",
        "VLA",
        "Manipulation",
        "Reasoning",
        "Vision",
        "Video",
        "Benchmark"
      ],
      "confidence": "ì¤‘",
      "adoption_complexity": "ì¤‘",
      "practical_relevance": "ë¡œë´‡ ìë™í™” ì‹œìŠ¤í…œ êµ¬ì¶• ì‹œ, ì‚¬ì „ í•™ìŠµëœ ì›”ë“œ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ê°•í™” í•™ìŠµ ê¸°ë°˜ ë¡œë´‡ ì œì–´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³ , ë‹¤ì–‘í•œ ì‘ì—… í™˜ê²½ì— ì‰½ê²Œ ì ì‘í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë° í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.",
      "recommended_actions": [
        "GigaBrain-0.5 ë° RAMP êµ¬í˜„ ì„¸ë¶€ ì‚¬í•­ í™•ì¸",
        "ìì²´ ë¡œë´‡ íŒ” í™˜ê²½ì—ì„œ RAMP ì ìš© ê°€ëŠ¥ì„± ê²€í† ",
        "ì›”ë“œ ëª¨ë¸ ì‚¬ì „ í•™ìŠµì— ì‚¬ìš©í•  ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ í™•ì¥ ì „ëµ ì—°êµ¬"
      ],
      "risks_and_limits": [
        "ì›”ë“œ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì „ì²´ ì‹œìŠ¤í…œ ì„±ëŠ¥ì— í° ì˜í–¥",
        "ì‹¤ì œ í™˜ê²½ì—ì„œì˜ ì¼ë°˜í™” ì„±ëŠ¥ ì¶”ê°€ ê²€ì¦ í•„ìš”"
      ],
      "uncertainty_note": "ì´ˆë¡ ì •ë³´ë§Œìœ¼ë¡œëŠ” GigaBrain-0.5M* ëª¨ë¸ì˜ êµ¬ì²´ì ì¸ êµ¬ì¡° ë° í•™ìŠµ ë°©ë²•, ê·¸ë¦¬ê³  ë°ì´í„°ì…‹ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ë¥¼ íŒŒì•…í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.",
      "repeated_in_window": false,
      "repeated_days_ago": null
    }
  ],
  "week": "2026-W08",
  "source_date": "2026-02-13"
}