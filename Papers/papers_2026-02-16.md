# 2026-02-16 Daily Papers (Top 5)

## 1. [Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs](https://huggingface.co/papers/2602.10388)
**Upvotes**: 195 | **도입 난이도**: 중 | **신뢰도**: 중
**arXiv**: https://arxiv.org/abs/2602.10388

**태그**: RAG

![Thumbnail](images/Less_is_Enough_Synthesizing_Diverse_Data_in_Feature_Space_of_LLMs_img.jpg)


---

## 2. [SQuTR: A Robustness Benchmark for Spoken Query to Text Retrieval under Acoustic Noise](https://huggingface.co/papers/2602.12783)
**Upvotes**: 131 | **도입 난이도**: 중 | **신뢰도**: 중
**arXiv**: https://arxiv.org/abs/2602.12783

**태그**: RAG, Benchmark, Evaluation

![Thumbnail](images/SQuTR_A_Robustness_Benchmark_for_Spoken_Query_to_Text_Retrieval_under_Acoustic_Noise_img.jpg)


---

## 3. [MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs](https://huggingface.co/papers/2602.12705)
**Upvotes**: 54 | **도입 난이도**: 중 | **신뢰도**: 중
**arXiv**: https://arxiv.org/abs/2602.12705

**태그**: Agent, RAG, Reasoning, Multimodal, Vision, Benchmark, Evaluation

![Thumbnail](images/MedXIAOHE_A_Comprehensive_Recipe_for_Building_Medical_MLLMs_img.jpg)


---

## 4. [Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception](https://huggingface.co/papers/2602.11858)
**Upvotes**: 50 | **도입 난이도**: 중 | **신뢰도**: 상
**arXiv**: https://arxiv.org/abs/2602.11858

**태그**: MLLM, Distillation, Fine-grained Perception, VQA, Zooming, Agent, Reasoning, Multimodal, Vision, Benchmark, Evaluation, Inference

![Thumbnail](images/Zooming_without_Zooming_Region-to-Image_Distillation_for_Fine-Grained_Multimodal_Perception_img.jpg)

### 📌 한 줄 요약
MLLM의 미세한 인식 능력 향상을 위해 추론 시점에 zooming을 사용하는 대신, region-to-image distillation을 통해 학습 시점에 zooming 효과를 내도록 하여 inference latency를 줄이고 성능을 향상시켰다.

### 🔑 핵심 포인트
- Region-to-Image Distillation을 통한 MLLM의 fine-grained perception 성능 향상
- Zooming을 학습 단계에 통합하여 inference latency 감소
- Fine-grained perception 평가를 위한 새로운 벤치마크 ZoomBench 제시

### 🧑‍💻 개발자 관점
MLLM 기반 서비스를 개발할 때, fine-grained perception 성능이 중요한 경우 (예: GUI 자동화, 의료 영상 분석 등) inference latency 증가 없이 성능 향상을 기대할 수 있다.

### 🚀 실무 적용 아이디어
- 제공된 github repository를 통해 pre-trained 모델 사용해보기
- ZoomBench 데이터셋을 이용하여 모델 성능 평가 및 개선
- 자체 데이터셋에 Region-to-Image Distillation 적용해보기

### ⚠️ 리스크/한계
- Distillation 과정에서 정보 손실 가능성
- 새로운 벤치마크 ZoomBench의 일반화 가능성

### 📝 초록 기반 상세 설명
Multimodal Large Language Models (MLLMs)은 넓은 시각적 이해에 강하지만, 미세한 인식 능력은 부족하다. 기존의 'Thinking-with-Images' 방법은 추론 과정에서 반복적인 zooming을 통해 이 문제를 해결하려 했으나, latency가 높았다. 본 논문에서는 Region-to-Image Distillation을 제안하여 zooming을 학습 단계에서 처리함으로써, agentic zooming의 이점을 단일 forward pass로 내재화한다. 구체적으로, 고품질 VQA 데이터를 생성하기 위해 미세하게 잘린 영역을 확대하고, 이 region-grounded supervision을 전체 이미지에 distillation한다. 이 방법은 도구 사용 없이도 미세한 인식 능력을 향상시키며, ZoomBench라는 새로운 벤치마크를 통해 성능을 검증한다.


---

## 5. [OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence](https://huggingface.co/papers/2602.08683)
**Upvotes**: 38 | **도입 난이도**: 중 | **신뢰도**: 중
**arXiv**: https://arxiv.org/abs/2602.08683

**태그**: Vision, LLM, Compression, Codec, Sparsity, RAG, Reasoning, Multimodal, Video, Benchmark, Optimization

![Thumbnail](images/OneVision-Encoder_Codec-Aligned_Sparsity_as_a_Foundational_Principle_for_Multimodal_Intelligence_img.jpg)

### 📌 한 줄 요약
동영상 코덱의 정보 이론적 원리를 활용하여 시각적 정보를 압축하고 LLM에 통합함으로써, 기존 Vision 백본 모델 대비 더 적은 연산량과 데이터로 더 높은 성능을 달성하는 새로운 시각 인코더 아키텍처를 제시합니다.

### 🔑 핵심 포인트
- 코덱 기반의 희소성 연산으로 효율적인 시각 정보 처리
- 3D RoPE를 활용한 공간-시간 정보 통합
- 대규모 클러스터 판별 학습을 통한 객체 영속성 및 움직임 역학 학습

### 🧑‍💻 개발자 관점
비디오/이미지 처리 성능 향상을 위해, 코덱의 정보 압축 원리를 모델 설계에 반영하는 새로운 접근 방식을 제공합니다. LLM 기반의 멀티모달 시스템에서 비전 모델의 효율성을 극대화하는 데 도움이 될 수 있습니다.

### 🚀 실무 적용 아이디어
- 자체 데이터셋에 OV-Encoder 적용하여 성능 및 효율성 비교
- 기존 비전 모델에 코덱 기반 희소성 연산 도입 시도
- 3D RoPE를 활용하여 공간-시간 정보 처리 성능 향상 실험

### ⚠️ 리스크/한계
- 특정 유형의 비디오 데이터에 편향될 가능성
- 코덱 패치화 과정에서 중요한 정보 손실 가능성

### 📝 초록 기반 상세 설명
기존 비전 모델은 시각적 정보의 중복성을 고려하지 않고 모든 픽셀에 대해 동일한 연산을 수행하여 비효율적인 문제가 있었습니다. 본 논문에서는 동영상 코덱의 정보 이론적 원리에 착안하여, 정보 엔트로피가 높은 영역에 집중하는 OneVision-Encoder (OV-Encoder)를 제안합니다. OV-Encoder는 코덱 패치화를 통해 입력의 3.1%-25% 영역만을 처리하고, 3D RoPE를 사용하여 공간 및 시간 정보를 통합합니다. 대규모 클러스터 판별 학습을 통해 객체 영속성과 움직임 역학을 공동으로 학습한 결과, OV-Encoder는 Qwen3-ViT 및 SigLIP2와 같은 강력한 비전 백본 모델을 능가하는 성능을 보였으며, 특히 비디오 이해 작업에서 평균 4.1%의 성능 향상을 달성했습니다.


---

