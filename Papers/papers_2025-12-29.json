[
    {
        "rank": 1,
        "title": "InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion",
        "url": "https://huggingface.co/papers/2512.17504",
        "upvotes": 87,
        "abstract": "Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.",
        "korean_summary": "InsertAnywhere는 4D 장면 기하학 이해와 확산 모델을 결합하여, 가려짐 및 조명 효과를 현실적으로 처리하며 기하학적으로 일관되고 시각적으로 충실한 비디오 객체 삽입(VOI)을 달성하는 새로운 프레임워크이다.",
        "key_points": [
            "장면 기하학을 재구성하고 시간적 일관성 및 가려짐 처리를 보장하여 기하학적으로 일관된 객체 배치를 위한 4D 인식 마스크 생성 모듈을 도입했다.",
            "확산 기반 비디오 생성 모델을 확장하여 삽입된 객체뿐만 아니라 주변의 조명, 음영 등 국부적인 환경 변화까지 공동으로 합성한다.",
            "지도 학습을 위해 기존 ROSE 데이터셋을 조명 인식이 가능한 합성 데이터셋(ROSE++)으로 변환하고, 객체 제거 영상, 객체 포함 영상, VLM 참조 이미지 삼중항을 구성했다."
        ],
        "image_url": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17504.png",
        "local_image_path": "images/InsertAnywhere_Bridging_4D_Scene_Geometry_and_Diffusion_Models_for_Realistic_Video_Object_Insertion_img.jpg",
        "local_figures": []
    },
    {
        "rank": 2,
        "title": "Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding",
        "url": "https://huggingface.co/papers/2512.17220",
        "upvotes": 84,
        "abstract": "Humans understand long and complex texts by relying on a holistic semantic representation of the content. This global view helps organize prior knowledge, interpret new information, and integrate evidence dispersed across a document, as revealed by the Mindscape-Aware Capability of humans in psychology. Current Retrieval-Augmented Generation (RAG) systems lack such guidance and therefore struggle with long-context tasks. In this paper, we propose Mindscape-Aware RAG (MiA-RAG), the first approach that equips LLM-based RAG systems with explicit global context awareness. MiA-RAG builds a mindscape through hierarchical summarization and conditions both retrieval and generation on this global semantic representation. This enables the retriever to form enriched query embeddings and the generator to reason over retrieved evidence within a coherent global context. We evaluate MiA-RAG across diverse long-context and bilingual benchmarks for evidence-based understanding and global sense-making. It consistently surpasses baselines, and further analysis shows that it aligns local details with a coherent global representation, enabling more human-like long-context retrieval and reasoning.",
        "korean_summary": "긴 문맥 이해 개선을 위해, 인간의 총체적 이해 능력을 모방하여 계층적 요약을 통해 전역적 맥락(Mindscape)을 구축하고 이를 검색과 생성 모두에 적용하는 MiA-RAG 방법론을 제안하며, 이는 우수한 성능을 입증한다.",
        "key_points": [
            "기존 RAG 시스템의 긴 문맥 이해력 한계를 극복하기 위해 인간의 '마인드스케이프 인식 능력(Mindscape-Aware Capability)'에서 영감을 얻어 전역적 문맥 인식의 필요성을 강조한다.",
            "MiA-RAG는 계층적 요약(hierarchical summarization)을 통해 전역적 의미 표현(mindscape)을 구축하며, 이 정보를 검색(retrieval)과 생성(generation) 과정 모두에 명시적인 조건(conditioning)으로 활용한다.",
            "이 접근 방식은 검색기(retriever)가 풍부한 쿼리 임베딩을 형성하고 생성기(generator)가 일관된 전역적 맥락 내에서 근거를 추론할 수 있게 하여, 긴 문맥 및 이중 언어 벤치마크에서 기존 기준선(baseline)보다 지속적으로 우수한 성능을 달성한다."
        ],
        "image_url": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17220.png",
        "local_image_path": "images/Mindscape-Aware_Retrieval_Augmented_Generation_for_Improved_Long_Context_Understanding_img.jpg",
        "local_figures": []
    },
    {
        "rank": 3,
        "title": "MAI-UI Technical Report: Real-World Centric Foundation GUI Agents",
        "url": "https://huggingface.co/papers/2512.22047",
        "upvotes": 25,
        "abstract": "The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent-user interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI addresses these issues with a unified methodology: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device-cloud collaboration system routes execution by task state, and an online RL framework with advanced optimizations to scale parallel environments and context length. MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation. On grounding benchmarks, it reaches 73.5% on ScreenSpot-Pro, 91.3% on MMBench GUI L2, 70.9% on OSWorld-G, and 49.2% on UI-Vision, surpassing Gemini-3-Pro and Seed1.8 on ScreenSpot-Pro. On mobile GUI navigation, it sets a new SOTA of 76.7% on AndroidWorld, surpassing UI-Tars-2, Gemini-2.5-Pro and Seed1.8. On MobileWorld, MAI-UI obtains 41.7% success rate, significantly outperforming end-to-end GUI models and competitive with Gemini-3-Pro based agentic frameworks. Our online RL experiments show significant gains from scaling parallel environments from 32 to 512 (+5.2 points) and increasing environment step budget from 15 to 50 (+4.3 points). Finally, the native device-cloud collaboration system improves on-device performance by 33%, reduces cloud model calls by over 40%, and preserves user privacy.",
        "korean_summary": "현실 세계 배포를 위한 기반 GUI 에이전트인 MAI-UI는 2B부터 235B까지 다양한 크기로 개발되었으며, 자체 진화 데이터 파이프라인, 네이티브 기기-클라우드 협업 시스템, 확장된 온라인 RL 프레임워크를 통해 GUI 벤치마크에서 새로운 최고 성능을 확립했다.",
        "key_points": [
            "MAI-UI는 사용자 상호작용 및 MCP 도구 호출을 포함하는 자체 진화 데이터 파이프라인과 작업 상태 기반 실행을 위한 네이티브 기기-클라우드 협업 시스템을 도입하여 현실 세계에서의 배포 문제를 해결한다.",
            "GUI 그라운딩 벤치마크(ScreenSpot-Pro 73.5%) 및 모바일 GUI 내비게이션 벤치마크(AndroidWorld 76.7%)에서 Gemini-3-Pro 및 Seed1.8을 능가하는 새로운 최고 성능(SOTA)을 달성했다.",
            "온라인 RL 환경의 병렬 처리 확장(32개→512개)을 통해 5.2%p 성능 향상을 보였으며, 기기-클라우드 협업 시스템을 통해 온디바이스 성능 33% 개선 및 클라우드 모델 호출 40% 이상 감소 효과를 얻었다."
        ],
        "image_url": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22047.png",
        "local_image_path": "images/MAI-UI_Technical_Report_Real-World_Centric_Foundation_GUI_Agents_img.jpg",
        "local_figures": []
    },
    {
        "rank": 4,
        "title": "UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture",
        "url": "https://huggingface.co/papers/2512.21675",
        "upvotes": 22,
        "abstract": "Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding.",
        "korean_summary": "본 연구는 미적 요소, 품질, 구조, 질감 등 지각 수준의 이미지 이해를 위한 통합 벤치마크(UniPercept-Bench)를 구축하고, DAPT 및 RL 기반으로 훈련된 강력한 기준 모델 UniPercept가 기존 MLLM을 능가하며 텍스트-이미지 생성의 보상 모델로 활용될 수 있음을 입증합니다.",
        "key_points": [
            "심미성, 품질, 구조, 질감에 걸친 지각 수준 이미지 이해를 위한 통일된 벤치마크 프레임워크 UniPercept-Bench를 제시하고, 이를 평가할 대규모 데이터셋과 계층적 정의 시스템을 구축했다.",
            "Domain-Adaptive Pre-Training(도메인 적응형 사전 학습) 및 Task-Aligned RL(작업 정렬 강화 학습)을 통해 훈련되어 Visual Rating(VR) 및 Visual Question Answering(VQA) 작업 전반에서 강력한 일반화 성능을 보이는 기준 모델 UniPercept를 개발했다.",
            "UniPercept는 기존 MLLM보다 지각 수준 이해에서 우수한 성능을 보였으며, 텍스트-이미지 생성 모델을 위한 플러그 앤 플레이 방식의 보상 모델로 기능할 수 있다."
        ],
        "image_url": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21675.png",
        "local_image_path": "images/UniPercept_Towards_Unified_Perceptual-Level_Image_Understanding_across_Aesthetics,_Quality,_Structur_img.jpg",
        "local_figures": []
    },
    {
        "rank": 5,
        "title": "TimeBill: Time-Budgeted Inference for Large Language Models",
        "url": "https://huggingface.co/papers/2512.21859",
        "upvotes": 16,
        "abstract": "Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. However, the auto-regressive generation process of LLMs makes it challenging to model and estimate the end-to-end execution time. Furthermore, existing efficient inference methods based on a fixed key-value (KV) cache eviction ratio struggle to adapt to varying tasks with diverse time budgets, where an improper eviction ratio may lead to incomplete inference or a drop in response performance. In this paper, we propose TimeBill, a novel time-budgeted inference framework for LLMs that balances the inference efficiency and response performance. To be more specific, we propose a fine-grained response length predictor (RLP) and an execution time estimator (ETE) to accurately predict the end-to-end execution time of LLMs. Following this, we develop a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on execution time prediction and the given time budget. Finally, through extensive experiments, we demonstrate the advantages of TimeBill in improving task completion rate and maintaining response performance under various overrun strategies.",
        "korean_summary": "TimeBill은 LLM이 시간 임계 시스템에서 주어진 시간 예산 내에 정확한 응답을 생성하도록 돕기 위해, 실행 시간을 예측하고 이에 맞춰 KV 캐시 제거 비율을 동적으로 조절하는 새로운 시간 예산 기반 추론 프레임워크이다.",
        "key_points": [
            "LLM의 자동 회귀적 생성 특성으로 인해 어려운 종단 간 실행 시간 모델링 문제를 해결하고, 시간 예산 내 응답 완료를 보장하는 것을 목표로 한다.",
            "응답 길이 예측기(RLP)와 실행 시간 예측기(ETE)를 포함하여 LLM의 총 실행 시간을 정밀하게 예측하는 구성 요소를 제안한다.",
            "예측된 실행 시간과 주어진 시간 예산을 기반으로 키-값(KV) 캐시 제거 비율을 적응적으로 조정하여 추론 효율성과 응답 성능의 균형을 맞춘다."
        ],
        "image_url": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21859.png",
        "local_image_path": "images/TimeBill_Time-Budgeted_Inference_for_Large_Language_Models_img.jpg",
        "local_figures": []
    }
]