[
    {
        "rank": 1,
        "title": "InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion",
        "url": "https://huggingface.co/papers/2512.17504",
        "upvotes": 72,
        "abstract": "Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.",
        "korean_summary": "비디오 객체 삽입(VOI)의 기하학적 일관성과 시각적 충실도를 높이기 위해 4D 장면 기하학 기반 마스크 생성과 확산 모델을 결합한 InsertAnywhere 프레임워크를 제안한다.",
        "key_points": [
            "4D 인지 마스크 생성 모듈을 사용하여 장면 기하학을 재구성하고, 시간적 일관성과 가림(occlusion) 일관성을 유지하며 사용자 지정 객체 위치를 프레임별로 일관되게 전파한다.",
            "확산 기반 비디오 생성 모델을 확장하여 삽입된 객체뿐만 아니라 조명(illumination) 및 음영(shading)과 같은 주변 환경의 국부적인 변화를 공동으로 합성하여 시각적 사실성을 극대화한다.",
            "지도 학습을 위해 기존 ROSE 객체 제거 데이터셋을 조명 인지 합성 데이터셋(ROSE++)으로 변환하였으며, 객체 제거 영상, 객체 포함 영상, VLM 생성 참조 이미지의 삼중쌍(triplets) 형태로 구축하였다."
        ],
        "image_url": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17504.png",
        "local_image_path": "images/InsertAnywhere_Bridging_4D_Scene_Geometry_and_Diffusion_Models_for_Realistic_Video_Object_Insertion_img.jpg",
        "local_figures": []
    },
    {
        "rank": 2,
        "title": "Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding",
        "url": "https://huggingface.co/papers/2512.17220",
        "upvotes": 69,
        "abstract": "Humans understand long and complex texts by relying on a holistic semantic representation of the content. This global view helps organize prior knowledge, interpret new information, and integrate evidence dispersed across a document, as revealed by the Mindscape-Aware Capability of humans in psychology. Current Retrieval-Augmented Generation (RAG) systems lack such guidance and therefore struggle with long-context tasks. In this paper, we propose Mindscape-Aware RAG (MiA-RAG), the first approach that equips LLM-based RAG systems with explicit global context awareness. MiA-RAG builds a mindscape through hierarchical summarization and conditions both retrieval and generation on this global semantic representation. This enables the retriever to form enriched query embeddings and the generator to reason over retrieved evidence within a coherent global context. We evaluate MiA-RAG across diverse long-context and bilingual benchmarks for evidence-based understanding and global sense-making. It consistently surpasses baselines, and further analysis shows that it aligns local details with a coherent global representation, enabling more human-like long-context retrieval and reasoning.",
        "korean_summary": "인간의 '마인드스케이프 인식 능력'에서 영감을 받아, 계층적 요약으로 구축된 전역적 의미 표현을 검색 및 생성에 활용하여 장문 이해 능력을 향상시킨 MiA-RAG 시스템을 제안한다.",
        "key_points": [
            "기존 RAG 시스템의 장문 이해 한계를 극복하기 위해 인간이 사용하는 총체적 의미 표현 능력(Mindscape-Aware Capability)을 도입했다.",
            "MiA-RAG는 계층적 요약을 통해 '마인드스케이프'(전역적 문맥)를 명시적으로 구축하고, 이 정보를 사용하여 검색(질의 임베딩 강화)과 생성(일관된 추론) 모두를 조건화한다.",
            "다양한 장문 및 이중 언어 벤치마크에서 기준선 성능을 일관되게 능가하며, 국소적 세부 사항을 일관된 전역적 표현과 일치시켜 인간과 유사한 장문 추론 능력을 입증했다."
        ],
        "image_url": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17220.png",
        "local_image_path": "images/Mindscape-Aware_Retrieval_Augmented_Generation_for_Improved_Long_Context_Understanding_img.jpg",
        "local_figures": []
    },
    {
        "rank": 3,
        "title": "MAI-UI Technical Report: Real-World Centric Foundation GUI Agents",
        "url": "https://huggingface.co/papers/2512.22047",
        "upvotes": 21,
        "abstract": "The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent-user interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI addresses these issues with a unified methodology: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device-cloud collaboration system routes execution by task state, and an online RL framework with advanced optimizations to scale parallel environments and context length. MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation. On grounding benchmarks, it reaches 73.5% on ScreenSpot-Pro, 91.3% on MMBench GUI L2, 70.9% on OSWorld-G, and 49.2% on UI-Vision, surpassing Gemini-3-Pro and Seed1.8 on ScreenSpot-Pro. On mobile GUI navigation, it sets a new SOTA of 76.7% on AndroidWorld, surpassing UI-Tars-2, Gemini-2.5-Pro and Seed1.8. On MobileWorld, MAI-UI obtains 41.7% success rate, significantly outperforming end-to-end GUI models and competitive with Gemini-3-Pro based agentic frameworks. Our online RL experiments show significant gains from scaling parallel environments from 32 to 512 (+5.2 points) and increasing environment step budget from 15 to 50 (+4.3 points). Finally, the native device-cloud collaboration system improves on-device performance by 33%, reduces cloud model calls by over 40%, and preserves user privacy.",
        "korean_summary": "MAI-UI는 고급 데이터 파이프라인, 장치-클라우드 협업 시스템 및 온라인 RL 프레임워크를 통해 현실적인 배포 문제를 해결하고, GUI 그라운딩 및 모바일 내비게이션에서 새로운 SOTA를 달성한 기반 GUI 에이전트 제품군이다.",
        "key_points": [
            "MAI-UI는 2B부터 235B-A22B까지 다양한 규모의 에이전트군을 제시하며, 사용자 상호작용 및 MCP 도구 호출을 포함하는 자체 진화형 데이터 파이프라인과 네이티브 장치-클라우드 협업 시스템을 통해 현실적인 배포 문제를 해결한다.",
            "GUI 그라운딩(ScreenSpot-Pro 73.5%, OSWorld-G 70.9% 등) 및 모바일 GUI 내비게이션(AndroidWorld 76.7%) 벤치마크에서 새로운 SOTA를 수립했으며, Gemini-3-Pro 및 Seed1.8과 같은 경쟁 모델들을 능가하는 성능을 보였다.",
            "병렬 환경 확장(32개→512개)을 통한 온라인 RL 프레임워크의 최적화로 성능을 크게 향상시켰으며, 네이티브 장치-클라우드 협업 시스템을 통해 온디바이스 성능 33% 향상 및 클라우드 모델 호출 40% 이상 절감 효과를 달성했다."
        ],
        "image_url": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22047.png",
        "local_image_path": "images/MAI-UI_Technical_Report_Real-World_Centric_Foundation_GUI_Agents_img.jpg",
        "local_figures": []
    },
    {
        "rank": 4,
        "title": "UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture",
        "url": "https://huggingface.co/papers/2512.21675",
        "upvotes": 20,
        "abstract": "Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding.",
        "korean_summary": "MLLM의 지각 수준 이미지 이해 능력 향상을 위해 심미성, 품질, 구조, 질감을 통합적으로 다루는 벤치마크(UniPercept-Bench)와 태스크 정렬 RL로 학습된 강력한 베이스라인 모델(UniPercept)을 제안합니다.",
        "key_points": [
            "심미성, 품질, 구조, 질감 전반에 걸친 지각 수준 이미지 이해를 위한 통합 프레임워크인 UniPercept-Bench를 구축하고 계층적 정의 시스템 및 대규모 데이터셋을 확립함.",
            "도메인 적응형 사전 학습(DAPT)과 태스크 정렬 강화 학습(Task-Aligned RL)을 적용하여 시각적 평가(VR)와 시각적 질의응답(VQA) 모두에서 강력한 일반화 성능을 보이는 UniPercept 베이스라인 모델을 개발함.",
            "UniPercept은 기존 MLLM 대비 지각 수준 이해에서 우수한 성능을 보이며, 텍스트-이미지 생성 모델을 위한 플러그 앤 플레이 보상 모델로 활용될 수 있음."
        ],
        "image_url": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21675.png",
        "local_image_path": "images/UniPercept_Towards_Unified_Perceptual-Level_Image_Understanding_across_Aesthetics,_Quality,_Structur_img.jpg",
        "local_figures": []
    },
    {
        "rank": 5,
        "title": "ProEdit: Inversion-based Editing From Prompts Done Right",
        "url": "https://huggingface.co/papers/2512.22118",
        "upvotes": 12,
        "abstract": "Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on source information, which negatively affects the edits in the target image (e.g., failing to change the subject's atributes like pose, number, or color as instructed). In this work, we propose ProEdit to address this issue both in the attention and the latent aspects. In the attention aspect, we introduce KV-mix, which mixes KV features of the source and the target in the edited region, mitigating the influence of the source image on the editing region while maintaining background consistency. In the latent aspect, we propose Latents-Shift, which perturbs the edited region of the source latent, eliminating the influence of the inverted latent on the sampling. Extensive experiments on several image and video editing benchmarks demonstrate that our method achieves SOTA performance. In addition, our design is plug-and-play, which can be seamlessly integrated into existing inversion and editing methods, such as RF-Solver, FireFlow and UniEdit.",
        "korean_summary": "ProEdit은 역변환 기반 편집에서 원본 이미지에 대한 과도한 의존성을 해결하기 위해 어텐션 영역의 KV-mix와 잠재 공간의 Latents-Shift를 제안하여 이미지 및 비디오 편집 벤치마크에서 SOTA 성능을 달성한 방법론이다.",
        "key_points": [
            "기존 역변환 기반 편집 방법론이 원본 정보에 과도하게 의존하여 대상 객체의 속성(자세, 수, 색상 등) 변화를 제대로 반영하지 못하는 문제를 해결한다.",
            "어텐션 측면에서 KV-mix를 도입하여 편집 영역 내에서 원본 및 목표 피처의 KV를 혼합함으로써 배경 일관성을 유지하면서도 원본 이미지의 영향을 줄인다.",
            "잠재 공간 측면에서 Latents-Shift를 제안하여 원본 잠재 벡터의 편집 영역을 교란시키고, 샘플링 과정에서 역변환된 잠재 벡터의 간섭 영향을 제거한다."
        ],
        "image_url": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22118.png",
        "local_image_path": "images/ProEdit_Inversion-based_Editing_From_Prompts_Done_Right_img.jpg",
        "local_figures": []
    }
]