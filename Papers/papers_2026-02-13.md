# 2026-02-13 Daily Papers (Top 5)

## 1. [The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies](https://huggingface.co/papers/2602.09877)
**Upvotes**: 181 | **도입 난이도**: 상 | **신뢰도**: 중

**태그**: Agent, Self-Evolution, Safety, LLM

![Thumbnail](images/The_Devil_Behind_Moltbook_Anthropic_Safety_is_Always_Vanishing_in_Self-Evolving_AI_Societies_img.jpg)

### 📌 한 줄 요약
자체 진화하는 AI 에이전트 사회는 안전 불변성을 유지하기 어렵다는 것을 이론 및 실험적으로 입증하여, 외부 감독이나 새로운 안전 메커니즘의 필요성을 강조합니다.

### 🔑 핵심 포인트
- 자체 진화, 완전 격리, 안전 불변성의 삼중고를 증명
- 정보 이론적 프레임워크를 사용하여 안전을 정량화
- 실제 에이전트 시스템에서 안전 침해 현상 확인

### 🧑‍💻 개발자 관점
LLM 기반 에이전트 시스템을 구축할 때 자체 진화 과정에서 안전 문제가 발생할 수 있음을 인지하고, 외부 감독이나 안전 메커니즘을 고려해야 합니다.

### 🚀 바로 실험할 액션
- 에이전트 시스템의 안전 정렬을 측정하기 위한 정보 이론적 지표 연구
- 자체 진화 과정에서 안전 침해를 감지하는 모니터링 시스템 개발
- 안전을 유지하면서 자체 진화할 수 있는 새로운 아키텍처 탐색

### ⚠️ 리스크/한계
- 안전의 정보 이론적 정의가 실제 안전 문제와 완전히 일치하지 않을 수 있음
- 제안된 해결책이 자체 진화의 효율성을 저해할 수 있음

### 🧭 불확실성 메모
초록 정보만으로는 실험 설정 및 결과의 구체적인 내용을 파악하기 어렵습니다.

### 📝 초록 기반 상세 설명
LLM 기반의 다중 에이전트 시스템은 확장 가능한 집단 지능 및 자체 진화에 대한 가능성을 제시하지만, 완전한 폐쇄 루프 내에서 안전 정렬을 유지하며 지속적인 자체 개선을 달성하는 것은 어려운 과제입니다. 본 연구에서는 정보 이론적 프레임워크를 통해 안전을 인간 가치 분포로부터의 발산 정도로 공식화하고, 고립된 자체 진화가 통계적 맹점을 유발하여 시스템의 안전 정렬이 불가역적으로 저하됨을 이론적으로 증명합니다. 실제 에이전트 커뮤니티 및 자체 진화 시스템에서의 실험 결과를 통해 이러한 안전 침해 현상을 확인하고, 안전 문제를 완화하기 위한 해결책을 제시합니다. 본 연구는 자체 진화 AI 사회의 근본적인 한계를 밝히고, 증상 중심의 안전 패치에서 벗어나 내재적인 동적 위험에 대한 원리적인 이해를 촉구합니다.


---

## 2. [Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models](https://huggingface.co/papers/2602.12036)
**Upvotes**: 86 | **도입 난이도**: 중 | **신뢰도**: 중

**태그**: Agent, Reinforcement Learning, Prompting, Reasoning, Evaluation

![Thumbnail](images/Composition-RL_Compose_Your_Verifiable_Prompts_for_Reinforcement_Learning_of_Large_Language_Models_img.jpg)

### 📌 한 줄 요약
Composition-RL은 쉬운 프롬프트를 활용하여 LLM의 추론 능력을 향상시키는 간단하면서도 효과적인 강화 학습 방법입니다.

### 🔑 핵심 포인트
- 정답률 1인 쉬운 프롬프트를 활용하여 RL 효율성 증대
- 여러 문제를 조합하여 새로운 프롬프트 생성
- 커리큘럼 학습을 통해 성능 향상

### 🧑‍💻 개발자 관점
LLM 기반 애플리케이션 개발 시, Composition-RL을 통해 적은 데이터로도 모델의 추론 능력을 향상시켜 개발 비용을 절감하고 성능을 개선할 수 있습니다.

### 🚀 바로 실험할 액션
- 기존 RL 학습 파이프라인에 Composition-RL 적용하여 성능 변화 확인
- 다양한 도메인의 프롬프트 조합을 통해 특정 작업 성능 향상 가능성 탐색
- 커리큘럼 학습 전략을 적용하여 학습 효율성 개선

### ⚠️ 리스크/한계
- 프롬프트 조합 방식에 따라 성능 저하 가능성 존재
- 특정 도메인 또는 작업에 대한 일반화 성능 검증 필요

### 🧭 불확실성 메모
초록 정보만으로는 Composition-RL의 실제 성능 향상 정도 및 적용 가능성에 대한 정확한 판단이 어렵습니다.

### 📝 초록 기반 상세 설명
최근 대규모 언어 모델의 강화 학습 (RL) 연구는 검증 가능한 보상을 사용하는 RL (RLVR)에 집중하고 있지만, 학습 데이터의 비효율성과 확장 비용 문제가 있습니다. 특히, 정답률이 1인 쉬운 프롬프트가 증가하면서 학습 데이터의 효율성이 떨어집니다. 이를 해결하기 위해, Composition-RL은 여러 문제를 조합하여 새로운 검증 가능한 질문을 생성하고, 이를 RL 학습에 활용합니다. 실험 결과, 4B에서 30B에 이르는 다양한 모델 크기에서 Composition-RL이 기존 RL 방식보다 추론 능력을 향상시켰습니다. 또한, 학습 과정에서 조합 깊이를 점진적으로 증가시키는 커리큘럼 방식을 통해 성능을 더욱 향상시킬 수 있으며, 다른 도메인의 프롬프트를 조합하여 도메인 간 RL에도 효과적입니다.


---

## 3. [DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing](https://huggingface.co/papers/2602.12205)
**Upvotes**: 69 | **도입 난이도**: 중 | **신뢰도**: 중

**태그**: Vision, Multimodal, Generation, Editing, RAG, Reasoning, Benchmark, Safety

![Thumbnail](images/DeepGen_1.0_A_Lightweight_Unified_Multimodal_Model_for_Advancing_Image_Generation_and_Editing_img.jpg)

### 📌 한 줄 요약
5B 파라미터의 경량 multimodal 모델 DeepGen 1.0을 공개, 더 큰 모델 대비 뛰어난 성능과 효율성을 제공하여 이미지 생성 및 편집 연구의 접근성을 높임.

### 🔑 핵심 포인트
- 5B 파라미터의 경량 unified multimodal 모델 DeepGen 1.0 제안
- Stacked Channel Bridging (SCB)을 통한 semantic 이해 및 fine-grained 제어 능력 향상
- 데이터 중심 학습 전략 (Alignment Pre-training, Joint Supervised Fine-tuning, RL with MR-GRPO)을 통한 성능 향상

### 🧑‍💻 개발자 관점
리소스 제약이 있는 환경에서도 고성능 이미지 생성 및 편집 모델을 사용할 수 있도록 하며, 모델 크기 때문에 접근하기 어려웠던 multimodal 연구에 기여할 수 있습니다.

### 🚀 바로 실험할 액션
- DeepGen 1.0 모델을 다운로드하여 이미지 생성 및 편집 task에 적용해보기
- 제공된 학습 코드를 분석하여 SCB 구조 및 학습 전략 이해하기
- 자체 데이터셋을 사용하여 DeepGen 1.0 모델 fine-tuning해보기

### ⚠️ 리스크/한계
- 50M 데이터셋으로 학습되었으므로, 특정 도메인에 대한 일반화 성능이 낮을 수 있음
- 경량 모델이므로, 매우 복잡한 시나리오에서는 성능이 제한적일 수 있음

### 🧭 불확실성 메모
초록만으로는 구체적인 모델 구조 및 학습 데이터셋에 대한 정보가 제한적이므로, 실제 적용 시 성능 차이가 발생할 수 있습니다.

### 📝 초록 기반 상세 설명
최근 이미지 생성 및 편집을 위한 unified multimodal 모델은 모델 크기가 커 학습 및 배포 비용이 높다는 문제가 있습니다. 본 논문에서는 5B 파라미터의 경량 모델인 DeepGen 1.0을 제안하며, 이는 더 큰 모델과 비교하여 경쟁력 있는 성능을 보여줍니다. 제안하는 Stacked Channel Bridging (SCB)은 VLM 레이어에서 계층적 특징을 추출하고 'think token'과 융합하여 생성 모델에 구조화된 지침을 제공합니다. 또한, Alignment Pre-training, Joint Supervised Fine-tuning, Reinforcement Learning with MR-GRPO로 구성된 데이터 중심 학습 전략을 사용하여 성능을 향상시켰습니다. DeepGen 1.0은 HunyuanImage (80B) 대비 WISE에서 28%, Qwen-Image-Edit (27B) 대비 UniREditBench에서 37% 높은 성능을 달성했습니다.


---

## 4. [Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation](https://huggingface.co/papers/2602.12125)
**Upvotes**: 55 | **도입 난이도**: 중 | **신뢰도**: 중

**태그**: Distillation, RL, On-Policy, Code Generation, Reasoning

![Thumbnail](images/Learning_beyond_Teacher_Generalized_On-Policy_Distillation_with_Reward_Extrapolation_img.jpg)

### 📌 한 줄 요약
On-policy distillation의 reward scaling factor 조정 및 reference model 변경을 통해 학생 모델의 성능을 teacher 성능 이상으로 향상시키는 방법론 G-OPD 제시.

### 🔑 핵심 포인트
- OPD를 일반화한 G-OPD 프레임워크 제시
- Reward scaling factor를 조정한 ExOPD를 통해 teacher 성능 능가
- Teacher의 사전 학습 모델을 활용한 reward correction 방법 제안

### 🧑‍💻 개발자 관점
모델 경량화 및 성능 향상을 위해 distillation을 사용하는 개발자에게 teacher 모델 이상의 성능을 얻을 수 있는 새로운 방법을 제시하며, 특히 도메인 지식 통합 과정에서 효과적이다.

### 🚀 바로 실험할 액션
- Reward scaling factor를 조정하며 distillation 실험 진행
- Strong-to-weak distillation 시 teacher의 사전 학습 모델을 reference model로 활용 시도
- 서로 다른 도메인에서 학습된 모델들을 ExOPD로 통합하는 실험 진행

### ⚠️ 리스크/한계
- Reward correction을 위한 teacher의 사전 학습 모델 접근 필요
- G-OPD의 효과는 특정 task (math reasoning, code generation)에 한정될 수 있음

### 🧭 불확실성 메모
초록 정보만을 바탕으로 작성되었으므로, 실험 환경 및 세부 설정에 따라 결과가 달라질 수 있다.

### 📝 초록 기반 상세 설명
On-policy distillation (OPD)은 학생 모델의 성능 향상에 효과적이지만, 기존 방식은 reward와 KL regularization의 가중치가 고정되어 있다는 한계가 있다. 본 연구에서는 OPD를 일반화한 Generalized On-Policy Distillation (G-OPD) 프레임워크를 제안하여, reference model과 reward scaling factor를 유연하게 조정할 수 있도록 하였다. 특히 reward scaling factor를 1보다 크게 설정하는 reward extrapolation (ExOPD) 방식이 다양한 teacher-student 크기 조합에서 성능 향상을 가져왔으며, 도메인 전문가 지식을 통합하는 과정에서 teacher 모델을 능가하는 결과를 얻었다. 또한, strong-to-weak distillation 환경에서 teacher의 사전 학습 모델을 reference model로 사용하여 reward signal을 개선하는 방법도 제시하였다. 다만 이 방법은 teacher 모델에 대한 추가적인 정보와 계산 비용이 필요하다.


---

## 5. [MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models](https://huggingface.co/papers/2602.10934)
**Upvotes**: 45 | **도입 난이도**: 중 | **신뢰도**: 중

**태그**: Audio, Transformer, Tokenizer, TTS, ASR, RAG, Distillation

![Thumbnail](images/MOSS-Audio-Tokenizer_Scaling_Audio_Tokenizers_for_Future_Audio_Foundation_Models_img.jpg)

### 📌 한 줄 요약
순수 Transformer 기반의 대규모 오디오 토크나이저(MOSS-Audio-Tokenizer)를 개발하여 다양한 오디오 도메인에서 SOTA 성능을 달성하고, 오디오 기반 파운데이션 모델의 새로운 가능성을 제시합니다.

### 🔑 핵심 포인트
- 순수 Transformer 기반의 end-to-end 오디오 토크나이저 아키텍처(CAT) 제안
- 3백만 시간의 오디오 데이터로 사전 학습된 16억 파라미터 규모의 MOSS-Audio-Tokenizer 개발
- TTS 및 ASR 성능 향상을 통해 오디오 파운데이션 모델로서의 가능성 입증

### 🧑‍💻 개발자 관점
MOSS-Audio-Tokenizer는 개발자들이 오디오 데이터를 활용한 다양한 애플리케이션(TTS, ASR 등)을 구축할 때 기반 모델로 활용할 수 있으며, 특히 Transformer 기반 아키텍처의 확장 가능성을 보여주어 자체 모델 개발에 영감을 줄 수 있습니다.

### 🚀 바로 실험할 액션
- MOSS-Audio-Tokenizer를 다운로드하여 자체 데이터셋에 적용해보기
- 제공된 API를 활용하여 간단한 오디오 처리 파이프라인 구축해보기
- CAT 아키텍처를 기반으로 새로운 오디오 처리 모델 개발 시도해보기

### ⚠️ 리스크/한계
- 대규모 모델이므로 학습 및 추론에 많은 컴퓨팅 자원이 필요할 수 있습니다.
- 3백만 시간 데이터셋의 편향이 모델 성능에 영향을 미칠 수 있습니다.

### 🧭 불확실성 메모
초록만으로는 모델의 구체적인 구현 방법과 학습 데이터셋의 상세 정보 확인이 어렵습니다.

### 📝 초록 기반 상세 설명
최근 오디오 처리 및 생성을 위한 discrete audio tokenizer 연구가 활발하지만, 기존 방법들은 pretrained encoder, semantic distillation, CNN 기반 아키텍처 등 고정된 inductive bias로 인해 성능 향상에 제한이 있었습니다. 본 논문에서는 encoder, quantizer, decoder를 end-to-end로 학습하는 순수 Transformer 기반의 CAT(Causal Audio Tokenizer with Transformer) 아키텍처를 제안합니다. CAT 아키텍처를 기반으로 16억 개의 파라미터와 3백만 시간 분량의 오디오 데이터로 사전 학습된 대규모 오디오 토크나이저인 MOSS-Audio-Tokenizer를 개발했습니다. 실험 결과, MOSS-Audio-Tokenizer는 다양한 비트레이트에서 기존 코덱을 능가하며, 모델 크기가 커질수록 성능이 향상되는 것을 확인했습니다. 또한, MOSS-Audio-Tokenizer를 활용하여 순수 autoregressive TTS 모델을 개발하여 기존 비-autoregressive 및 cascaded 시스템을 능가하는 성능을 달성했으며, 보조 encoder 없이도 경쟁력 있는 ASR 성능을 보여줍니다.


---

