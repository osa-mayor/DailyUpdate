[
    {
        "rank": 1,
        "title": "Latent Implicit Visual Reasoning",
        "url": "https://huggingface.co/papers/2512.21218",
        "upvotes": 57,
        "abstract": "While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.",
        "korean_summary": "대규모 멀티모달 모델(LMM)의 텍스트 중심적 시각 추론 한계를 극복하고자, 명시적인 지도 없이 잠재적인 시각적 추론 토큰을 발견하고 활용하여 다양한 시각 중심 작업에서 최고 성능을 달성하는 태스크 비종속적 접근 방식을 제시한다.",
        "key_points": [
            "기존 LMM은 언어 기반 추론에 의존하여 시각 중심적인 추론 작업 수행에 한계가 있으며, 중간 시각 단계에 대한 명시적 지도(예: 깊이 맵, 이미지 자르기)는 비싸고 일반화가 어렵다.",
            "수동으로 정의된 지도 없이 LMM이 잠재적인 시각적 추론 토큰(Visual Reasoning Tokens)을 자체적으로 발견하고 사용하도록 훈련시키는 태스크 불특정적(task-agnostic) 메커니즘을 제안한다.",
            "이 토큰들은 이미지를 전역적으로 참조하며 작업에 적응적으로(task-adaptive) 재인코딩하여 관련 시각 정보를 추출하며, 직접적인 미세 조정보다 우수하고 다양한 시각 중심 작업에서 최첨단 결과를 달성한다."
        ],
        "image_url": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21218.png",
        "local_image_path": "images/Latent_Implicit_Visual_Reasoning_img.jpg",
        "local_figures": [
            "images/Latent_Implicit_Visual_Reasoning_fig_1.jpeg"
        ]
    },
    {
        "rank": 2,
        "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning",
        "url": "https://huggingface.co/papers/2512.20605",
        "upvotes": 54,
        "abstract": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.",
        "korean_summary": "자기회귀 모델의 비효율적인 토큰별 행동 생성 문제를 극복하고자, 잔차 스트림 활성화(residual stream activations)를 제어하는 고차 비인과적 시퀀스 모델을 도입하여 시간적 추상화를 달성하고, 이를 '내부 RL'이라 명명한 강화 학습 방법으로 활용하여 계층적 RL의 효율성을 획기적으로 향상시킨다.",
        "key_points": [
            "표준 강화 학습(RL)의 비효율성을 극복하기 위해, 자기회귀 모델의 내부 표현(internal representations)을 활용하여 시간적으로 추상화된 행동을 탐색하고 실행하는 프레임워크를 제안한다.",
            "기본 모델의 잔차 스트림 활성화를 제어하는 고차원, 비인과적 시퀀스 모델을 도입하며, 이 모델은 긴 활성화 시퀀스를 행동적으로 의미 있는 장기적인 '내부 컨트롤러'와 학습된 종료 조건으로 압축한다.",
            "내부 컨트롤러를 직접 강화하는 '내부 RL(Internal RL)' 프로세스는 표준 RL 미세 조정이 실패하는 희소 보상 환경에서도 효과적으로 작동함을 입증하여, 파운데이션 모델 내에서 계층적 강화 학습(HRL)을 실현하는 유망한 길을 제시한다."
        ],
        "image_url": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20605.png",
        "local_image_path": "images/Emergent_temporal_abstractions_in_autoregressive_models_enable_hierarchical_reinforcement_learning_img.jpg",
        "local_figures": []
    },
    {
        "rank": 3,
        "title": "Spatia: Video Generation with Updatable Spatial Memory",
        "url": "https://huggingface.co/papers/2512.15716",
        "upvotes": 25,
        "abstract": "Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.",
        "korean_summary": "스패이셔(Spatia)는 3D 장면 포인트 클라우드를 지속 가능한 공간 메모리로 명시적으로 보존하고 Visual SLAM을 통해 이를 업데이트함으로써, 장기적인 공간 및 시간적 일관성을 강화하는 비디오 생성 프레임워크이다.",
        "key_points": [
            "기존 비디오 생성 모델의 장기 일관성 문제를 해결하기 위해 3D 장면 포인트 클라우드를 영구적인 공간 메모리(Persistent Spatial Memory)로 활용한다.",
            "생성된 비디오 클립을 공간 메모리에 조건화하고 Visual SLAM을 통해 메모리를 지속적으로 업데이트하여 기하학적 기반의 일관성 유지 능력을 확보한다.",
            "정적 요소와 동적 요소를 명확히 분리(Dynamic-Static Disentanglement)하여 공간 일관성을 향상시키며, 명시적 카메라 제어 및 3D 인식 상호작용 편집과 같은 응용이 가능하다."
        ],
        "image_url": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15716.png",
        "local_image_path": "images/Spatia_Video_Generation_with_Updatable_Spatial_Memory_img.jpg",
        "local_figures": []
    },
    {
        "rank": 4,
        "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models",
        "url": "https://huggingface.co/papers/2512.19995",
        "upvotes": 13,
        "abstract": "Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.",
        "korean_summary": "대규모 언어 모델(LLM)의 추론 과정을 체계적으로 분석하기 위해 Schoenfeld의 에피소드 이론 기반 ThinkARM 프레임워크를 도입했으며, 이는 모델 간의 구조적 사고 역학 및 정확도에 영향을 미치는 명시적인 추론 단계를 밝혀낸다.",
        "key_points": [
            "Schoenfeld의 에피소드 이론을 중간 규모의 렌즈로 채택하고, 추론 과정을 기능적 단계(분석, 탐색, 구현, 검증 등)로 추상화하는 ThinkARM 프레임워크를 도입하여 토큰 수준 분석의 한계를 극복했다.",
            "이 추상화는 수학적 문제 해결 과정에서 토큰 수준 분석으로는 드러나지 않는, 추론 모델과 비추론 모델 간의 재현 가능한 사고 역학 및 구조적 차이를 명확하게 보여준다.",
            "진단적 사례 연구 결과, '탐색(Exploration)' 단계가 정확도와 관련된 중요한 분기점 역할을 하며, 효율 중심의 방법론이 응답을 일괄적으로 줄이는 대신 평가적 피드백 단계를 선택적으로 억제함을 발견했다."
        ],
        "image_url": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19995.png",
        "local_image_path": "images/Schoenfeld's_Anatomy_of_Mathematical_Reasoning_by_Language_Models_img.jpg",
        "local_figures": []
    },
    {
        "rank": 5,
        "title": "How Much 3D Do Video Foundation Models Encode?",
        "url": "https://huggingface.co/papers/2512.19949",
        "upvotes": 8,
        "abstract": "Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.",
        "korean_summary": "대규모 영상 데이터로 학습된 비디오 기반 모델(VidFMs)의 3D 인코딩 능력을 측정하는 프레임워크를 제안하며, 최신 비디오 생성 모델이 3D 데이터 없이도 전문 3D 모델을 능가하는 강력한 3D 이해력을 자연스럽게 습득했음을 밝혀낸 연구이다.",
        "key_points": [
            "다양한 비디오 기반 모델(VidFMs)의 특징에서 얕은 읽기(shallow read-outs)를 통해 3D 속성을 추정함으로써 모델의 3D 인지도를 측정하는 최초의 모델 불가지론적(model-agnostic) 프레임워크를 제시함.",
            "3D 데이터에 명시적으로 훈련되지 않은 최신 비디오 생성 모델들이 객체와 장면에 대한 강력한 3D 이해력을 자연스럽게 인코딩함을 발견함.",
            "VidFMs의 3D 이해도가 특정 3D 작업을 위해 훈련된 대규모 전문가 모델의 성능을 능가할 수 있음을 입증하며, 확장 가능한 3D 모델 구축을 위한 중요한 관찰 결과를 제공함."
        ],
        "image_url": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19949.png",
        "local_image_path": "images/How_Much_3D_Do_Video_Foundation_Models_Encode_img.jpg",
        "local_figures": [
            "images/How_Much_3D_Do_Video_Foundation_Models_Encode_fig_1.png"
        ]
    }
]